{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Treponema genome analysis workflow - Grillova et al. 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tested software versions and operation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The workflows has been tested on Linux machine (Ubuntu 16.04) with Python 2.7.6, Python 3.4.3 using Conda 4.5.11, Jupyter notebook 5.7.2 and bash_kernel 0.7.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux BioDA-server 4.4.0-148-generic #174-Ubuntu SMP Tue May 7 12:20:14 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux\n",
      "(.env) No LSB modules are available.\n",
      "Distributor ID:\tUbuntu\n",
      "Description:\tUbuntu 16.04.6 LTS\n",
      "Release:\t16.04\n",
      "Codename:\txenial\n",
      "(.env) (.env) /opt/install/dir/anaconda/bin/python3\n",
      "(.env) Python 3.7.1\n",
      "(.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/bin/conda\n",
      "(.env) conda 4.6.14\n",
      "(.env) Jupyter notebook 4.4.1\n",
      "(.env) The program 'pip3' is currently not installed. To run 'pip3' please ask your administrator to install the package 'python3-pip'\n",
      "bash_kernel\n",
      "(.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "uname -a\n",
    "lsb_release -a\n",
    "export PATH=/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/bin:$PATH # In case we use different version of Conda than the system-wide installation\n",
    "which python3\n",
    "python3 --version\n",
    "which conda\n",
    "conda --version\n",
    "echo \"Jupyter notebook\" `jupyter notebook --version`\n",
    "echo \"bash_kernel\" `pip3 show bash_kernel | grep Version`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## The analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we can activate the environment and check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(.env) (treponema) (.env) (treponema) (.env) \n",
      "     active environment : treponema\n",
      "    active env location : /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema\n",
      "            shell level : 1\n",
      "       user config file : /mnt/nfs/home/325073/000000-My_Documents/VM-home/.condarc\n",
      " populated config files : /mnt/nfs/home/325073/000000-My_Documents/VM-home/.condarc\n",
      "          conda version : 4.6.14\n",
      "    conda-build version : not installed\n",
      "         python version : 2.7.16.final.0\n",
      "       base environment : /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2  (read only)\n",
      "           channel URLs : https://conda.anaconda.org/conda-forge/linux-64\n",
      "                          https://conda.anaconda.org/conda-forge/noarch\n",
      "                          https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/free/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/free/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://conda.anaconda.org/r/linux-64\n",
      "                          https://conda.anaconda.org/r/noarch\n",
      "                          https://conda.anaconda.org/bioconda/linux-64\n",
      "                          https://conda.anaconda.org/bioconda/noarch\n",
      "          package cache : /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/pkgs\n",
      "                          /mnt/nfs/home/325073/000000-My_Documents/VM-home/.conda/pkgs\n",
      "       envs directories : /mnt/nfs/home/325073/000000-My_Documents/VM-home/.conda/envs\n",
      "                          /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/4.6.14 requests/2.21.0 CPython/2.7.16 Linux/4.4.0-148-generic ubuntu/16.04.6 glibc/2.23\n",
      "                UID:GID : 100134:100006\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "(treponema) (.env) # packages in environment at /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_r-mutex                  1.0.0               anacondar_1  \n",
      "asn1crypto                0.24.0                py27_1003    conda-forge\n",
      "augustus                  3.3             pl526hcfae127_4    bioconda\n",
      "backports                 1.0                        py_2    conda-forge\n",
      "backports.functools_lru_cache 1.5                        py_1    conda-forge\n",
      "backports_abc             0.5                        py_1    conda-forge\n",
      "bamtools                  2.4.1                         1    bioconda\n",
      "bbmap                     38.22                h14c3975_1    bioconda\n",
      "bcftools                  1.4.1                         0    bioconda\n",
      "besst                     2.2.8                      py_2    bioconda\n",
      "bioconductor-biobase      2.42.0           r351h14c3975_1    bioconda\n",
      "bioconductor-biocgenerics 0.28.0                   r351_1    bioconda\n",
      "bioconductor-biocparallel 1.16.6           r351h1c2f66e_0    bioconda\n",
      "bioconductor-biostrings   2.50.2           r351h14c3975_0    bioconda\n",
      "bioconductor-delayedarray 0.8.0            r351h14c3975_0    bioconda\n",
      "bioconductor-genomeinfodb 1.18.1                   r351_0    bioconda\n",
      "bioconductor-genomeinfodbdata 1.2.1                    r351_0    bioconda\n",
      "bioconductor-genomicalignments 1.18.1           r351h14c3975_0    bioconda\n",
      "bioconductor-genomicranges 1.34.0           r351h14c3975_0    bioconda\n",
      "bioconductor-iranges      2.16.0           r351h14c3975_0    bioconda\n",
      "bioconductor-noiseq       2.26.1                   r351_0    bioconda\n",
      "bioconductor-rsamtools    1.34.0           r351hf484d3e_0    bioconda\n",
      "bioconductor-rtracklayer  1.42.1           r351h9d9f1b6_1    bioconda\n",
      "bioconductor-s4vectors    0.20.1           r351h14c3975_0    bioconda\n",
      "bioconductor-summarizedexperiment 1.12.0                   r351_0    bioconda\n",
      "bioconductor-xvector      0.22.0           r351h14c3975_0    bioconda\n",
      "bioconductor-zlibbioc     1.28.0           r351h14c3975_0    bioconda\n",
      "blas                      1.0                         mkl  \n",
      "blast                     2.7.1                h4422958_6    bioconda\n",
      "boost                     1.67.0           py27h3e44d54_0    conda-forge\n",
      "boost-cpp                 1.67.0               h3a22d5f_0    conda-forge\n",
      "busco                     3.0.2                    py27_8    bioconda\n",
      "bwa                       0.7.15                        1    bioconda\n",
      "bwidget                   1.9.11                        1  \n",
      "bz2file                   0.98                       py_0    conda-forge\n",
      "bzip2                     1.0.6             h14c3975_1002    conda-forge\n",
      "ca-certificates           2019.6.16            hecc5488_0    conda-forge\n",
      "cairo                     1.14.12           h80bd089_1005    conda-forge\n",
      "certifi                   2019.3.9                 py27_0    anaconda\n",
      "cffi                      1.12.3           py27h8022711_0    conda-forge\n",
      "chardet                   3.0.4                 py27_1003    conda-forge\n",
      "circos                    0.69.6                        5    bioconda\n",
      "click                     7.0                        py_0    conda-forge\n",
      "colormath                 3.0.0                      py_2    conda-forge\n",
      "cryptography              2.7              py27h72c5cf5_0    conda-forge\n",
      "curl                      7.64.1               hf8cf82a_0    conda-forge\n",
      "cutadapt                  1.18             py27h14c3975_1    bioconda\n",
      "cycler                    0.10.0                     py_1    conda-forge\n",
      "dbus                      1.13.2               h714fa37_1  \n",
      "decorator                 4.4.0                      py_0    conda-forge\n",
      "enum34                    1.1.6                 py27_1001    conda-forge\n",
      "expat                     2.2.5             hf484d3e_1002    conda-forge\n",
      "fastqc                    0.11.8                        1    bioconda\n",
      "font-ttf-dejavu-sans-mono 2.37                 h6964260_0  \n",
      "fontconfig                2.13.1            he4413a7_1000    conda-forge\n",
      "freebayes                 0.9.21.26                     0    bioconda\n",
      "freetype                  2.10.0               he983fc9_0    conda-forge\n",
      "functools32               3.2.3.2                    py_3    conda-forge\n",
      "future                    0.17.1                py27_1000    conda-forge\n",
      "futures                   3.2.0                 py27_1000    conda-forge\n",
      "gatk                      3.7                      py27_1    bioconda\n",
      "gettext                   0.19.8.1          hc5be6a0_1002    conda-forge\n",
      "giflib                    5.1.9                h516909a_0    conda-forge\n",
      "glib                      2.56.2            had28632_1001    conda-forge\n",
      "glimmerhmm                3.0.4                h2d50403_2    bioconda\n",
      "gmp                       6.1.2             hf484d3e_1000    conda-forge\n",
      "gnutls                    3.5.19               h2a4e5f8_1    conda-forge\n",
      "graphite2                 1.3.13            hf484d3e_1000    conda-forge\n",
      "gsl                       2.2.1                h0c605f7_3  \n",
      "gst-plugins-base          1.14.0               hbbd80ab_1  \n",
      "gstreamer                 1.14.0               hb453b48_1  \n",
      "harfbuzz                  1.9.0             he243708_1001    conda-forge\n",
      "hmmer                     3.2.1                hf484d3e_1    bioconda\n",
      "htslib                    1.9                  h4da6232_3    bioconda\n",
      "icu                       58.2              hf484d3e_1000    conda-forge\n",
      "idna                      2.8                   py27_1000    conda-forge\n",
      "intel-openmp              2019.4                      243  \n",
      "ipaddress                 1.0.22                     py_1    conda-forge\n",
      "jinja2                    2.10.1                     py_0    conda-forge\n",
      "joblib                    0.13.2                     py_0    conda-forge\n",
      "jpeg                      9c                h14c3975_1001    conda-forge\n",
      "kiwisolver                1.1.0            py27hc9558a2_0    conda-forge\n",
      "kraken2                   2.0.8_beta      pl526h6bb024c_0    bioconda\n",
      "krb5                      1.16.3            h05b26f9_1001    conda-forge\n",
      "libcurl                   7.64.1               hda55be3_0    conda-forge\n",
      "libdeflate                1.2                  h14c3975_0    bioconda\n",
      "libedit                   3.1.20170329      hf8c457e_1001    conda-forge\n",
      "libffi                    3.2.1             he1b5a44_1006    conda-forge\n",
      "libgcc                    7.2.0                h69d50b8_2    conda-forge\n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \n",
      "libgd                     2.2.5             h0d07dcb_1005    conda-forge\n",
      "libgfortran               3.0.0                         1    conda-forge\n",
      "libgfortran-ng            7.3.0                hdf63c60_0  \n",
      "libiconv                  1.15              h516909a_1005    conda-forge\n",
      "libidn2                   2.1.1                h14c3975_0    conda-forge\n",
      "libpng                    1.6.37               hed695b0_0    conda-forge\n",
      "libssh2                   1.8.2                h22169c7_2    conda-forge\n",
      "libstdcxx-ng              9.1.0                hdf63c60_0  \n",
      "libtiff                   4.0.10            h57b8799_1003    conda-forge\n",
      "libunistring              0.9.10               h14c3975_0    conda-forge\n",
      "libuuid                   2.32.1            h14c3975_1000    conda-forge\n",
      "libwebp                   1.0.2                h576950b_1    conda-forge\n",
      "libxcb                    1.13              h14c3975_1002    conda-forge\n",
      "libxml2                   2.9.9                h13577e0_0    conda-forge\n",
      "lp_solve                  5.5.2.5           h14c3975_1001    conda-forge\n",
      "lz4-c                     1.8.3             he1b5a44_1001    conda-forge\n",
      "lzstring                  1.0.4                   py_1001    conda-forge\n",
      "make                      4.2.1             h14c3975_2004    conda-forge\n",
      "markdown                  2.6.11                     py_0    conda-forge\n",
      "markupsafe                1.1.1            py27h14c3975_0    conda-forge\n",
      "mathstats                 0.2.6.5                    py_0    bioconda\n",
      "matplotlib                2.2.3            py27h8a2030e_1    conda-forge\n",
      "matplotlib-base           2.2.3            py27h60b886d_1    conda-forge\n",
      "metis                     5.1.0             hf484d3e_1003    conda-forge\n",
      "mkl                       2019.4                      243  \n",
      "mkl_fft                   1.0.13           py27h516909a_1    conda-forge\n",
      "mkl_random                1.0.4            py27hf2d7682_0    conda-forge\n",
      "multiqc                   1.7                        py_4    bioconda\n",
      "ncurses                   6.1               hf484d3e_1002    conda-forge\n",
      "nettle                    3.3                           0    conda-forge\n",
      "networkx                  2.2                        py_1    conda-forge\n",
      "numpy                     1.16.4           py27h7e9f1db_0  \n",
      "numpy-base                1.16.4           py27hde5b4d6_0  \n",
      "openblas                  0.2.20                        8    conda-forge\n",
      "openjdk                   8.0.192           h14c3975_1003    conda-forge\n",
      "openssl                   1.1.1                h7b6447c_0    anaconda\n",
      "pango                     1.40.14           hf0c64fd_1003    conda-forge\n",
      "pcre                      8.43                 he6710b0_0  \n",
      "perl                      5.26.2            h516909a_1006    conda-forge\n",
      "perl-app-cpanminus        1.7044                  pl526_1    bioconda\n",
      "perl-archive-tar          2.32                    pl526_0    bioconda\n",
      "perl-autoloader           5.74                    pl526_2    bioconda\n",
      "perl-carp                 1.38                    pl526_3    bioconda\n",
      "perl-clone                0.41            pl526h14c3975_1    bioconda\n",
      "perl-compress-raw-bzip2   2.086           pl526hf484d3e_0    bioconda\n",
      "perl-compress-raw-zlib    2.086           pl526h6bb024c_1    bioconda\n",
      "perl-config-general       2.63                    pl526_0    bioconda\n",
      "perl-constant             1.33                    pl526_1    bioconda\n",
      "perl-cpan-meta            2.150010                pl526_0    bioconda\n",
      "perl-cpan-meta-requirements 2.140                   pl526_0    bioconda\n",
      "perl-cpan-meta-yaml       0.018                   pl526_0    bioconda\n",
      "perl-data-dumper          2.173                   pl526_0    bioconda\n",
      "perl-dbi                  1.642                   pl526_0    bioconda\n",
      "perl-digest-perl-md5      1.9                     pl526_1    bioconda\n",
      "perl-dynaloader           1.25                    pl526_1    bioconda\n",
      "perl-encode               2.88                    pl526_1    bioconda\n",
      "perl-exporter             5.72                    pl526_1    bioconda\n",
      "perl-exporter-tiny        1.002001                pl526_0    bioconda\n",
      "perl-extutils-cbuilder    0.280230                pl526_1    bioconda\n",
      "perl-extutils-makemaker   7.36                    pl526_1    bioconda\n",
      "perl-extutils-manifest    1.72                    pl526_0    bioconda\n",
      "perl-extutils-parsexs     3.35                    pl526_0    bioconda\n",
      "perl-file-path            2.16                    pl526_0    bioconda\n",
      "perl-file-temp            0.2304                  pl526_2    bioconda\n",
      "perl-font-ttf             1.06                    pl526_0    bioconda\n",
      "perl-gd                   2.71            pl526he860b03_0    bioconda\n",
      "perl-getopt-long          2.50                    pl526_1    bioconda\n",
      "perl-io-compress          2.086           pl526hf484d3e_0    bioconda\n",
      "perl-io-string            1.08                    pl526_3    bioconda\n",
      "perl-io-zlib              1.10                    pl526_2    bioconda\n",
      "perl-ipc-cmd              1.02                    pl526_0    bioconda\n",
      "perl-json-pp              4.02                    pl526_0    bioconda\n",
      "perl-list-moreutils       0.428                   pl526_1    bioconda\n",
      "perl-list-moreutils-xs    0.428                   pl526_0    bioconda\n",
      "perl-locale-maketext-simple 0.21                    pl526_2    bioconda\n",
      "perl-math-bezier          0.01                    pl526_1    bioconda\n",
      "perl-math-round           0.07                    pl526_1    bioconda\n",
      "perl-math-vecstat         0.08                    pl526_1    bioconda\n",
      "perl-module-build         0.4224                  pl526_3    bioconda\n",
      "perl-module-corelist      5.20190524              pl526_0    bioconda\n",
      "perl-module-implementation 0.09                    pl526_2    bioconda\n",
      "perl-module-load          0.32                    pl526_1    bioconda\n",
      "perl-module-load-conditional 0.68                    pl526_2    bioconda\n",
      "perl-module-metadata      1.000036                pl526_0    bioconda\n",
      "perl-module-runtime       0.016                   pl526_1    bioconda\n",
      "perl-number-format        1.75                    pl526_3    bioconda\n",
      "perl-params-check         0.38                    pl526_1    bioconda\n",
      "perl-params-validate      1.29            pl526h14c3975_1    bioconda\n",
      "perl-parent               0.236                   pl526_1    bioconda\n",
      "perl-pathtools            3.75            pl526h14c3975_1    bioconda\n",
      "perl-perl-ostype          1.010                   pl526_1    bioconda\n",
      "perl-readonly             2.05                    pl526_0    bioconda\n",
      "perl-regexp-common        2017060201              pl526_0    bioconda\n",
      "perl-scalar-list-utils    1.50            pl526h14c3975_0    bioconda\n",
      "perl-set-intspan          1.19                    pl526_1    bioconda\n",
      "perl-statistics-basic     1.6611                  pl526_2    bioconda\n",
      "perl-svg                  2.84                    pl526_0    bioconda\n",
      "perl-text-abbrev          1.02                    pl526_0    bioconda\n",
      "perl-text-format          0.59                    pl526_2    bioconda\n",
      "perl-text-parsewords      3.30                    pl526_0    bioconda\n",
      "perl-time-hires           1.9760          pl526h14c3975_1    bioconda\n",
      "perl-try-tiny             0.30                    pl526_1    bioconda\n",
      "perl-version              0.9924                  pl526_0    bioconda\n",
      "perl-xml-parser           2.44            pl526h4e0c4b3_7    bioconda\n",
      "perl-xsloader             0.24                    pl526_0    bioconda\n",
      "perl-yaml                 1.29                    pl526_0    bioconda\n",
      "picard                    2.9.2                         2    bioconda\n",
      "pigz                      2.3.4                         0    conda-forge\n",
      "pip                       19.1.1                   py27_0    conda-forge\n",
      "pixman                    0.34.0            h14c3975_1003    conda-forge\n",
      "popt                      1.16                          1    bioconda\n",
      "pthread-stubs             0.4               h14c3975_1001    conda-forge\n",
      "pycparser                 2.19                     py27_1    conda-forge\n",
      "pyopenssl                 19.0.0                   py27_0    conda-forge\n",
      "pyparsing                 2.4.0                      py_0    conda-forge\n",
      "pyqt                      5.6.0           py27h13b7fb3_1008    conda-forge\n",
      "pysam                     0.11.2.2                 py27_1    bioconda\n",
      "pysocks                   1.7.0                    py27_0    conda-forge\n",
      "python                    2.7.15            h721da81_1008    conda-forge\n",
      "python-dateutil           2.8.0                      py_0    conda-forge\n",
      "pytz                      2019.1                     py_0    conda-forge\n",
      "pyyaml                    5.1.1            py27h516909a_0    conda-forge\n",
      "qt                        5.6.3                h8bf5577_3  \n",
      "qualimap                  2.2.2b                        1    bioconda\n",
      "quast                     5.0.2           py27pl526ha92aebf_0    bioconda\n",
      "r                         3.5.1                 r351_1000    conda-forge\n",
      "r-assertthat              0.2.1            r351h6115d3f_0    conda-forge\n",
      "r-backports               1.1.4            r351hcdcec82_0    conda-forge\n",
      "r-base                    3.5.1                h391c2eb_5    conda-forge\n",
      "r-bh                      1.69.0_1         r351h6115d3f_0    conda-forge\n",
      "r-bitops                  1.0_6           r351h96ca727_1002    conda-forge\n",
      "r-boot                    1.3_22                   r351_0    conda-forge\n",
      "r-class                   7.3_15          r351h96ca727_1000    conda-forge\n",
      "r-cli                     1.1.0            r351h6115d3f_0    conda-forge\n",
      "r-cluster                 2.0.9            r351h9bbef5b_0    conda-forge\n",
      "r-codetools               0.2_16          r351h6115d3f_1000    conda-forge\n",
      "r-colorspace              1.4_1            r351hcdcec82_0    conda-forge\n",
      "r-crayon                  1.3.4           r351h6115d3f_1001    conda-forge\n",
      "r-digest                  0.6.19           r351h0357c0b_0    conda-forge\n",
      "r-fansi                   0.4.0           r351h96ca727_1000    conda-forge\n",
      "r-foreign                 0.8_71          r351h96ca727_1002    conda-forge\n",
      "r-formatr                 1.7               r35h6115d3f_0    conda-forge\n",
      "r-futile.logger           1.4.3           r351h6115d3f_1001    conda-forge\n",
      "r-futile.options          1.0.1           r351h6115d3f_1000    conda-forge\n",
      "r-getopt                  1.20.3                   r351_0    conda-forge\n",
      "r-ggplot2                 3.2.0             r35h6115d3f_0    conda-forge\n",
      "r-glue                    1.3.1            r351hcdcec82_0    conda-forge\n",
      "r-gtable                  0.3.0            r351h6115d3f_0    conda-forge\n",
      "r-kernsmooth              2.23_15         r351ha65eedd_1002    conda-forge\n",
      "r-labeling                0.3             r351h6115d3f_1001    conda-forge\n",
      "r-lambda.r                1.2.3           r351h6115d3f_1000    conda-forge\n",
      "r-lattice                 0.20_38         r351h96ca727_1000    conda-forge\n",
      "r-lazyeval                0.2.2            r351hcdcec82_0    conda-forge\n",
      "r-magrittr                1.5             r351h6115d3f_1001    conda-forge\n",
      "r-mass                    7.3_51.4         r351hcdcec82_0    conda-forge\n",
      "r-matrix                  1.2_17           r351hcdcec82_0    conda-forge\n",
      "r-matrixstats             0.54.0          r351h96ca727_1000    conda-forge\n",
      "r-mgcv                    1.8_28           r351hcdcec82_0    conda-forge\n",
      "r-munsell                 0.5.0           r351h6115d3f_1001    conda-forge\n",
      "r-nlme                    3.1_140          r351h9bbef5b_0    conda-forge\n",
      "r-nnet                    7.3_12          r351h96ca727_1002    conda-forge\n",
      "r-optparse                1.6.1            r351h6115d3f_0    conda-forge\n",
      "r-pillar                  1.4.1                h6115d3f_0    conda-forge\n",
      "r-pkgconfig               2.0.2           r351h6115d3f_1001    conda-forge\n",
      "r-plyr                    1.8.4           r351h29659fb_1002    conda-forge\n",
      "r-r6                      2.4.0            r351h6115d3f_0    conda-forge\n",
      "r-rcolorbrewer            1.1_2           r351h6115d3f_1001    conda-forge\n",
      "r-rcpp                    1.0.1            r351h0357c0b_0    conda-forge\n",
      "r-rcurl                   1.95_4.12        r351hcdcec82_0    conda-forge\n",
      "r-recommended             3.5.1                 r351_1001    conda-forge\n",
      "r-reshape2                1.4.3           r351h29659fb_1003    conda-forge\n",
      "r-rlang                   0.3.4            r351hcdcec82_0    conda-forge\n",
      "r-rpart                   4.1_15           r351hcdcec82_0    conda-forge\n",
      "r-scales                  1.0.0           r351h29659fb_1001    conda-forge\n",
      "r-snow                    0.4_3           r351h6115d3f_1000    conda-forge\n",
      "r-spatial                 7.3_11          r351h96ca727_1002    conda-forge\n",
      "r-stringi                 1.4.3            r351h0357c0b_0    conda-forge\n",
      "r-stringr                 1.4.0            r351h6115d3f_0    conda-forge\n",
      "r-survival                2.44_1.1         r351hcdcec82_0    conda-forge\n",
      "r-tibble                  2.1.3             r35hcdcec82_0    conda-forge\n",
      "r-utf8                    1.1.4           r351h96ca727_1000    conda-forge\n",
      "r-vctrs                   0.1.0           r351h96ca727_1000    conda-forge\n",
      "r-viridislite             0.3.0           r351h6115d3f_1001    conda-forge\n",
      "r-withr                   2.1.2           r351h6115d3f_1000    conda-forge\n",
      "r-xml                     3.98_1.20         r35hcdcec82_0    conda-forge\n",
      "r-zeallot                 0.1.0           r351h6115d3f_1000    conda-forge\n",
      "readline                  7.0               hf8c457e_1001    conda-forge\n",
      "reaper                    16.098               ha92aebf_2    bioconda\n",
      "requests                  2.22.0                   py27_0    conda-forge\n",
      "rsync                     3.1.3             h84994c4_1002    conda-forge\n",
      "samtools                  1.4.1                         0    bioconda\n",
      "scipy                     1.2.1            py27h7c811a0_0  \n",
      "seqtk                     1.3                  h84994c4_1    bioconda\n",
      "setuptools                41.0.1                   py27_0    conda-forge\n",
      "simplejson                3.8.1                    py27_0    bioconda\n",
      "singledispatch            3.4.0.3               py27_1000    conda-forge\n",
      "sip                       4.18.1          py27hf484d3e_1000    conda-forge\n",
      "six                       1.12.0                py27_1000    conda-forge\n",
      "snpeff                    4.3.1t                        2    bioconda\n",
      "spades                    3.13.0                        0    bioconda\n",
      "spectra                   0.0.11                     py_1    conda-forge\n",
      "sqlite                    3.28.0               h8b20d00_0    conda-forge\n",
      "subprocess32              3.5.4            py27h516909a_0    conda-forge\n",
      "suitesparse               5.2.0                h9e4a6bb_0  \n",
      "tbb                       2019.7               hc9558a2_0    conda-forge\n",
      "tk                        8.6.9             hed695b0_1002    conda-forge\n",
      "tktable                   2.10                 h14c3975_0  \n",
      "tornado                   5.1.1           py27h14c3975_1000    conda-forge\n",
      "urllib3                   1.24.3                   py27_0    conda-forge\n",
      "vcflib                    1.0.0_rc2            h56106d0_2    bioconda\n",
      "vcftools                  0.1.15               he941832_2    bioconda\n",
      "virtualenv                16.0.0                   py27_0    anaconda\n",
      "wget                      1.20.1               h90d6eec_0    conda-forge\n",
      "wheel                     0.33.4                   py27_0    conda-forge\n",
      "xopen                     0.7.0                      py_0    bioconda\n",
      "xorg-kbproto              1.0.7             h14c3975_1002    conda-forge\n",
      "xorg-libice               1.0.9             h516909a_1004    conda-forge\n",
      "xorg-libsm                1.2.3             h84519dc_1000    conda-forge\n",
      "xorg-libx11               1.6.7             h14c3975_1000    conda-forge\n",
      "xorg-libxau               1.0.9                h14c3975_0    conda-forge\n",
      "xorg-libxdmcp             1.1.3                h516909a_0    conda-forge\n",
      "xorg-libxext              1.3.4                h516909a_0    conda-forge\n",
      "xorg-libxrender           0.9.10            h516909a_1002    conda-forge\n",
      "xorg-renderproto          0.11.1            h14c3975_1002    conda-forge\n",
      "xorg-xextproto            7.3.0             h14c3975_1002    conda-forge\n",
      "xorg-xproto               7.0.31            h14c3975_1007    conda-forge\n",
      "xz                        5.2.4             h14c3975_1001    conda-forge\n",
      "yaml                      0.1.7             h14c3975_1001    conda-forge\n",
      "zlib                      1.2.11            h14c3975_1004    conda-forge\n",
      "zstd                      1.4.0                h3b9ef0a_0    conda-forge\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "#export PATH=/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/bin:$PATH # In case we use different version of Conda than the system-wide installation\n",
    "source activate treponema\n",
    "# In case we want to export the system settings and software versions\n",
    "conda info\n",
    "conda list\n",
    "# In case we modified the environment want to export it\n",
    "#conda env export > treponema_mod.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Running the workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we have to setup few variables which will be used throughout the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "INPUT_DIR=\"/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/raw\" # Directory with the raw paired input files in fastq.gz - first pair has to be named xxx_R1.fastq.gz and the second xxx_R2.fastq.gz otherwise you would have to change the input suffixes for the individual steps\n",
    "OUTPUT_DIR=\"/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/results\" # Directory where we want to save the results\n",
    "\n",
    "REFERENCE=\"/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/references/SS14.fa.gz\" # Bacteria reference genome\n",
    "HOST_GENOME=\"/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/references/GCF_000001405.36_GRCh38.p10_genomic.fna.gz\" # Host genome reference sequence; for human - ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.36_GRCh38.p10/GCF_000001405.36_GRCh38.p10_genomic.fna.gz\n",
    "\n",
    "THREADS=12 # Number of threads we will use in the analysis\n",
    "\n",
    "ADAPTER_R1=\"CTGTCTCTTATACACATCT\" # R1 3' adapeter, if any\n",
    "ADAPTER_R2=$ADAPTER_R1 # R2 3' adapter, if any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initial quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is always a good idea to run an initial quality check on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/raw/CW56_S1_R1_001.fastq.gz\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/raw/CW56_S1_R2_001.fastq.gz\n",
      "(treponema) (.env) (treponema) (.env) Started analysis of CW56_S1_R1_001.fastq.gz\n",
      "Started analysis of CW56_S1_R2_001.fastq.gz\n",
      "Approx 5% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 5% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 10% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 10% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 15% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 15% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 20% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 20% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 25% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 25% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 30% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 30% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 35% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 35% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 40% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 40% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 45% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 45% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 50% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 50% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 55% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 55% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 60% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 60% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 65% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 65% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 70% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 70% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 75% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 75% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 80% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 80% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 85% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 85% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 90% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 90% complete for CW56_S1_R2_001.fastq.gz\n",
      "Approx 95% complete for CW56_S1_R1_001.fastq.gz\n",
      "Approx 95% complete for CW56_S1_R2_001.fastq.gz\n",
      "Analysis complete for CW56_S1_R1_001.fastq.gz\n",
      "Unable to revert mtime: /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/fonts\n",
      "Unable to revert mtime: /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/fonts/modern\n",
      "Unable to revert mtime: /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/fonts/symbols\n",
      "Analysis complete for CW56_S1_R2_001.fastq.gz\n",
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:45: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  configs = yaml.load(f)\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:51: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  sp = yaml.load(f)\n",
      "[INFO   ]         multiqc : This is MultiQC v1.7\n",
      "[INFO   ]         multiqc : Template    : default\n",
      "[INFO   ]         multiqc : Searching '/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/results/qc/fastqc/raw/'\n",
      "[INFO   ]          fastqc : Found 2 reports\n",
      "[INFO   ]         multiqc : Compressing plot data\n",
      "[INFO   ]         multiqc : Report      : ../../home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/results/qc/fastqc/raw/multiqc_report.html\n",
      "[INFO   ]         multiqc : Data        : ../../home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/results/qc/fastqc/raw/multiqc_data\n",
      "[INFO   ]         multiqc : MultiQC complete\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir -p $OUTPUT_DIR/qc/fastqc/raw\n",
    "\n",
    "ls $INPUT_DIR/*.gz\n",
    "\n",
    "fastqc --threads $THREADS --outdir $OUTPUT_DIR/qc/fastqc/raw $INPUT_DIR/*.gz\n",
    "\n",
    "multiqc --outdir $OUTPUT_DIR/qc/fastqc/raw $OUTPUT_DIR/qc/fastqc/raw/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we are working with DNA data and aiming for the results including the polymorphisms we should perform a careful preprocessing to remove the adapter sequences and perform quality trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Now I am processing CW56_S1_R1_001.fastq.gz as first in a pair and CW56_S1_R2_001.fastq.gz as a second in a pair.\n",
      "Done processing CW56_S1_R1_001.fastq.gz and CW56_S1_R2_001.fastq.gz\n",
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:45: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  configs = yaml.load(f)\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:51: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  sp = yaml.load(f)\n",
      "[INFO   ]         multiqc : This is MultiQC v1.7\n",
      "[INFO   ]         multiqc : Template    : default\n",
      "[INFO   ]         multiqc : Searching '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/cutadapt/'\n",
      "[INFO   ]        cutadapt : Found 1 reports\n",
      "[INFO   ]         multiqc : Compressing plot data\n",
      "[INFO   ]         multiqc : Report      : ../results/qc/cutadapt/multiqc_report.html\n",
      "[INFO   ]         multiqc : Data        : ../results/qc/cutadapt/multiqc_data\n",
      "[INFO   ]         multiqc : MultiQC complete\n",
      "(treponema) (.env) (treponema) (.env) Started analysis of CW56_S1_R1.trimmed.fastq.gz\n",
      "Started analysis of CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 5% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 5% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 10% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 10% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 15% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 15% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 20% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 20% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 25% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 25% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 30% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 30% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 35% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 35% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 40% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 40% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 45% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 45% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 50% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 50% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 55% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 55% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 60% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 60% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 65% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 65% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 70% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 70% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 75% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 75% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 80% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 80% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 85% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 85% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 90% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 90% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Approx 95% complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Approx 95% complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "Analysis complete for CW56_S1_R1.trimmed.fastq.gz\n",
      "Analysis complete for CW56_S1_R2.trimmed.fastq.gz\n",
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:45: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  configs = yaml.load(f)\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:51: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  sp = yaml.load(f)\n",
      "[INFO   ]         multiqc : This is MultiQC v1.7\n",
      "[INFO   ]         multiqc : Template    : default\n",
      "[INFO   ]         multiqc : Searching '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/fastqc/preprocessed/'\n",
      "[INFO   ]          fastqc : Found 2 reports\n",
      "[INFO   ]         multiqc : Compressing plot data\n",
      "[INFO   ]         multiqc : Report      : ../results/qc/fastqc/preprocessed/multiqc_report.html\n",
      "[INFO   ]         multiqc : Data        : ../results/qc/fastqc/preprocessed/multiqc_data\n",
      "[INFO   ]         multiqc : MultiQC complete\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir -p $OUTPUT_DIR/data/preprocessed\n",
    "mkdir $OUTPUT_DIR/qc/cutadapt\n",
    "mkdir $OUTPUT_DIR/qc/fastqc/preprocessed\n",
    "\n",
    "cd $INPUT_DIR/\n",
    "\n",
    "for sample in *R1*.gz\n",
    "do\n",
    "    FORWARD=$sample\n",
    "    extension=\"${FORWARD##*R1}\"\n",
    "    REVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "    # Input file check\n",
    "    if [ ! -f ${FORWARD} ]; then\n",
    "    echo \"Input file not found! First in pair.\"; echo ${FORWARD}\n",
    "    fi\n",
    "    if [ ! -f ${REVERSE} ]; then\n",
    "    echo \"Input file not found! Second in pair.\"; echo ${REVERSE}\n",
    "    fi\n",
    "\n",
    "    echo \"Now I am processing ${FORWARD} as first in a pair and ${REVERSE} as a second in a pair.\"\n",
    "\n",
    "    cutadapt -a $ADAPTER_R1 -A $ADAPTER_R2 \\\n",
    "    --times 1 --quality-cutoff 15,15 --trim-n \\\n",
    "    --error-rate 0.10 -O 3 --minimum-length 35 --max-n 0 \\\n",
    "    --output $OUTPUT_DIR/data/preprocessed/${FORWARD%$extension}.trimmed.fastq.gz \\\n",
    "    --paired-output $OUTPUT_DIR/data/preprocessed/${REVERSE%$extension}.trimmed.fastq.gz \\\n",
    "    $FORWARD $REVERSE &>$OUTPUT_DIR/qc/cutadapt/${FORWARD%_R1${extension}}.cutadapt.out\n",
    "\n",
    "    echo \"Done processing $FORWARD and $REVERSE\"\n",
    "done\n",
    "\n",
    "multiqc --outdir $OUTPUT_DIR/qc/cutadapt $OUTPUT_DIR/qc/cutadapt/\n",
    "\n",
    "fastqc --threads $THREADS --outdir $OUTPUT_DIR/qc/fastqc/preprocessed $OUTPUT_DIR/data/preprocessed/*.gz\n",
    "\n",
    "multiqc --outdir $OUTPUT_DIR/qc/fastqc/preprocessed $OUTPUT_DIR/qc/fastqc/preprocessed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Host genome contamination removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Removal of the host genome DNA before the analysis speeds up the analysis and we will be working with much smaller files as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As is usuall, we have to generate a host genome DNA reference index. This is run just once for one reference and one BBMap version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/bbmap_index\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) java -Djava.library.path=/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/opt/bbmap-38.22-1/jni/ -ea -Xmx20g -cp /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/opt/bbmap-38.22-1/current/ align2.BBMap build=1 overwrite=true fastareadlen=500 ref=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/GCF_000001405.36_GRCh38.p10_genomic.fna.gz -Xmx20g path=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/bbmap_index/GCF_000001405.36_GRCh38.p10_genomic.fna.gz\n",
      "Executing align2.BBMap [build=1, overwrite=true, fastareadlen=500, ref=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/GCF_000001405.36_GRCh38.p10_genomic.fna.gz, -Xmx20g, path=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/bbmap_index/GCF_000001405.36_GRCh38.p10_genomic.fna.gz]\n",
      "Version 38.22\n",
      "\n",
      "No output file.\n",
      "Writing reference.\n",
      "Executing dna.FastaToChromArrays2 [/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/GCF_000001405.36_GRCh38.p10_genomic.fna.gz, 1, writeinthread=false, genscaffoldinfo=true, retain, waitforwriting=false, gz=true, maxlen=536670912, writechroms=true, minscaf=1, midpad=300, startpad=8000, stoppad=8000, nodisk=false]\n",
      "\n",
      "Set genScaffoldInfo=true\n",
      "Writing chunk 1\n",
      "Writing chunk 2\n",
      "Writing chunk 3\n",
      "Writing chunk 4\n",
      "Writing chunk 5\n",
      "Writing chunk 6\n",
      "Writing chunk 7\n",
      "Set genome to 1\n",
      "\n",
      "Loaded Reference:\t0.052 seconds.\n",
      "Loading index for chunk 1-7, build 1\n",
      "No index available; generating from reference genome: /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/bbmap_index/GCF_000001405.36_GRCh38.p10_genomic.fna.gz/ref/index/1/chr1-3_index_k13_c2_b1.block\n",
      "No index available; generating from reference genome: /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/bbmap_index/GCF_000001405.36_GRCh38.p10_genomic.fna.gz/ref/index/1/chr4-7_index_k13_c2_b1.block\n",
      "Indexing threads started for block 0-3\n",
      "Indexing threads started for block 4-7\n",
      "Indexing threads finished for block 0-3\n",
      "Indexing threads finished for block 4-7\n",
      "Generated Index:\t477.441 seconds.\n",
      "Finished Writing:\t19.012 seconds.\n",
      "No reads to process; quitting.\n",
      "\n",
      "Total time:     \t551.026 seconds.\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir $(dirname $HOST_GENOME)/bbmap_index\n",
    "echo $(dirname $HOST_GENOME)/bbmap_index\n",
    "\n",
    "# If the host genome index does not exist create it\n",
    "if [ ! -d \"$(dirname $HOST_GENOME)/bbmap_index/$(basename $HOST_GENOME)\" ]; then\n",
    "    mkdir $(dirname $HOST_GENOME)/bbmap_index/$(basename $HOST_GENOME)\n",
    "    bbmap.sh ref=$HOST_GENOME -Xmx20g path=$(dirname $HOST_GENOME)/bbmap_index/$(basename $HOST_GENOME)\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the index is done we can launch the host-genome removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Now I am processing CW56_S1_R1.trimmed.fastq.gz as first in a pair and CW56_S1_R2.trimmed.fastq.gz as a second in a pair with reference /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/GCF_000001405.36_GRCh38.p10_genomic.fna.gz.\n",
      "java -ea -Xmx12g -cp /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/opt/bbmap-38.22-1/current/ jgi.ReformatReads -Xmx12g verifypaired in=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1.clean.fastq.gz out1=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1_R1.clean.fastq.gz out2=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1_R2.clean.fastq.gz\n",
      "Executing jgi.ReformatReads [-Xmx12g, verifypaired, in=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1.clean.fastq.gz, out1=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1_R1.clean.fastq.gz, out2=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/data/preprocessed/CW56_S1_R2.clean.fastq.gz]\n",
      "\n",
      "Input is being processed as paired\n",
      "Input:                  \t9905136 reads          \t1399685204 bases\n",
      "Output:                 \t9905136 reads (100.00%) \t1399685204 bases (100.00%)\n",
      "\n",
      "Time:                         \t15.561 seconds.\n",
      "Reads Processed:       9905k \t636.52k reads/sec\n",
      "Bases Processed:       1399m \t89.95m bases/sec\n",
      "Names appear to be correctly paired.\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir $OUTPUT_DIR/qc/bbmap\n",
    "\n",
    "cd $OUTPUT_DIR/data/preprocessed/\n",
    "\n",
    "for sample in *R1*trimmed.fastq.gz\n",
    "do\n",
    "    FORWARD=$sample\n",
    "    extension=\"${FORWARD##*R1}\"\n",
    "    REVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "    # Input file check\n",
    "    if [ ! -f ${FORWARD} ]; then\n",
    "        echo \"Input file not found! First in pair.\"; echo ${FORWARD}\n",
    "    fi\n",
    "    if [ ! -f ${REVERSE} ]; then\n",
    "        echo \"Input file not found! Second in pair.\"; echo ${REVERSE}\n",
    "    fi\n",
    "\n",
    "    echo \"Now I am processing ${FORWARD} as first in a pair and ${REVERSE} as a second in a pair with reference $HOST_GENOME.\"\n",
    "\n",
    "    # Start mapping\n",
    "    bbmap.sh threads=$THREADS -Xmx25g minid=0.95 maxindel=3 bandwidthratio=0.16 \\\n",
    "    bandwidth=12 quickmatch fast minhits=2 path=$(dirname $HOST_GENOME)/bbmap_index/$(basename $HOST_GENOME) unpigz pigz \\\n",
    "    in=${FORWARD} in2=${REVERSE} outu=$OUTPUT_DIR/data/preprocessed/${FORWARD%_R1${extension}}.clean.fastq.gz \\\n",
    "    outm=$OUTPUT_DIR/data/preprocessed/${FORWARD%_R1${extension}}.dirty.fastq.gz &>$OUTPUT_DIR/qc/bbmap/${FORWARD%_R1${extension}}.bbmap.out # qtrim=rl trimq=10 untrim  # We already have preprocessed data, no need for this\n",
    "\n",
    "    # De-interleave\n",
    "    reformat.sh -Xmx12g verifypaired in=$OUTPUT_DIR/data/preprocessed/${FORWARD%_R1${extension}}.clean.fastq.gz \\\n",
    "    out1=$OUTPUT_DIR/data/preprocessed/${FORWARD%${extension}}.clean.fastq.gz out2=$OUTPUT_DIR/data/preprocessed/${REVERSE%${extension}}.clean.fastq.gz\n",
    "\n",
    "    # Remove the host genome mapped reads (usefull for mapping precision)\n",
    "    rm $OUTPUT_DIR/data/preprocessed/${FORWARD%_R1${extension}}.clean.fastq.gz\n",
    "    rm $OUTPUT_DIR/data/preprocessed/${FORWARD%_R1${extension}}.dirty.fastq.gz\n",
    "done\n",
    "\n",
    "mkdir $OUTPUT_DIR/qc/fastqc/clean\n",
    "\n",
    "fastqc --threads $THREADS --outdir $OUTPUT_DIR/qc/fastqc/clean $OUTPUT_DIR/data/preprocessed/*.clean.fastq.gz\n",
    "\n",
    "multiqc --outdir $OUTPUT_DIR/qc/fastqc/clean $OUTPUT_DIR/qc/fastqc/clean/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bacteria contamination scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before we start with the alignment we can quickly scan for possible bacterial contamination in our dataset. This scan uses default StrainSeeker database which is most likely outdated but StrainSeeker offers a possibility to generate your [own index](http://bioinfo.ut.ee/strainseeker/index.php?r=site/page&view=manual#database) for the scan with their builder script. One advantage over tools such as [Kraken2](https://ccb.jhu.edu/software/kraken2/) (a great tool) is that is consumes much less RAM. However, the latest releases of MiniKraken2 could be used as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before we can use StrainSeeker we have to download the database to scan. We can use the database provided at the StrainSeeker webpage or create our own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "STRAINSEEKER_DB=/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/references/strainseekerdb\n",
    "\n",
    "mkdir $STRAINSEEKER_DB\n",
    "cd $STRAINSEEKER_DB/\n",
    "wget http://bioinfo.ut.ee/strainseeker/executables/ss_db_w32_4324.tar.gz\n",
    "tar xvzf ss_db_w32_4324.tar.gz\n",
    "\n",
    "STRAINSEEKER_DB=$STRAINSEEKER_DB/ss_db_w32_4324"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have the database we can start the scan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/strainseeker’: File exists\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Working on sample CW56_S1_R1.clean.fastq.gz\n",
      "Subsampling\n",
      "Scanning\n",
      "Read tree info file...\n",
      "Getting k-mer counts from sample...\n",
      "Searching...\n",
      "------\n",
      "Reading tree info file: 0.0s\n",
      "Get k-mer counts from sample: 34.0s\n",
      "Search process: 9.0s\n",
      "Cleanup: 0.0s\n",
      "Database root-leaf count: 878\n",
      "TOTAL RUN TIME: 43.0s\n",
      "------\n",
      "Input files:\n",
      "CW56_S1_R1.clean.fastq.gz.sub\n",
      "Results saved in: /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/strainseeker/CW56_S1_R1.clean.seeker.txt\n",
      "Working on sample CW56_S1_R2.clean.fastq.gz\n",
      "Subsampling\n",
      "Scanning\n",
      "Read tree info file...\n",
      "Getting k-mer counts from sample...\n",
      "Searching...\n",
      "------\n",
      "Reading tree info file: 0.0s\n",
      "Get k-mer counts from sample: 33.0s\n",
      "Search process: 8.0s\n",
      "Cleanup: 0.0s\n",
      "Database root-leaf count: 878\n",
      "TOTAL RUN TIME: 41.0s\n",
      "------\n",
      "Input files:\n",
      "CW56_S1_R2.clean.fastq.gz.sub\n",
      "Results saved in: /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/strainseeker/CW56_S1_R2.clean.seeker.txt\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir $OUTPUT_DIR/qc/strainseeker\n",
    "\n",
    "cd $OUTPUT_DIR/data/preprocessed/\n",
    "\n",
    "for sample in *.clean.fastq.gz\n",
    "do \n",
    "    echo \"Working on sample $sample\"\n",
    "\n",
    "    echo \"Subsampling\"\n",
    "    seqtk sample -s100 $sample 1000000 > $sample.sub # Subsample fastq to 1M\n",
    "\n",
    "    echo \"Scanning\"\n",
    "    seeker.pl -i $sample.sub -d $STRAINSEEKER_DB -o $OUTPUT_DIR/qc/strainseeker/${sample%.fastq.gz}.seeker.txt\n",
    "\n",
    "    rm $sample.sub\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bacteria contamination scan - Kraken2 (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you have enough resources you can use [Kraken2](https://ccb.jhu.edu/software/kraken2/). It scans the most recent bacterial, viral and fungal databases (or their subselection) and evaluates the possible distributions of individual species. If you decided to use Kraken2, you have to build the Kraken2 database. Please note this might take a while and will consume quite a lof of [RAM](https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual#kraken-2-databases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) 06182019\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "KRAKEN2_DB=\"/mnt/nfs/home/323639/000000-My_Documents/VM-home/projects/honza/linda/pipeline/data/references/kraken2db\" # Directory with stored StrainSeeker database\n",
    "mkdir $KRAKEN2_DB\n",
    "\n",
    "DATE=$(date +'%m%d%Y') # Save current date -> database version\n",
    "echo $DATE\n",
    "\n",
    "kraken2-build --standard --threads $THREADS --db $KRAKEN2_DB/$DATE\n",
    "\n",
    "KRAKEN2_DB=$KRAKEN2_DB/06182019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have the database we can start the scan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(.env) (.env) (.env) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "THREADS=12\n",
    "OUTPUT_DIR=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results\n",
    "KRAKEN2_DB=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/kraken2db/06182019\n",
    "REFERENCE=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Now I am processing CW56_S1_R1.clean.fastq.gz as first in a pair and CW56_S1_R2.clean.fastq.gz as a second in a pair with database /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/kraken2db/06182019.\n",
      "Loading database information... done.\n",
      "4952568 sequences (1399.69 Mbp) processed in 39.405s (7541.1 Kseq/m, 2131.24 Mbp/m).\n",
      "  3344833 sequences classified (67.54%)\n",
      "  1607735 sequences unclassified (32.46%)\n",
      "[1] 60912\n",
      "[2] 60913\n",
      "[3] 60914\n",
      "[4] 60915\n",
      "(treponema) (.env) (treponema) (.env) [1]   Done                    gzip $output\n",
      "[3]-  Done                    gzip $output\n",
      "[4]+  Done                    gzip $output\n",
      "[2]+  Done                    gzip $output\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mkdir -p $OUTPUT_DIR/qc/kraken2/reads\n",
    "\n",
    "cd $OUTPUT_DIR/data/preprocessed/\n",
    "\n",
    "for sample in *R1*.clean.fastq.gz\n",
    "do\n",
    "    FORWARD=$sample\n",
    "    extension=\"${FORWARD##*R1}\"\n",
    "    REVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "    # Input file check\n",
    "    if [ ! -f ${FORWARD} ]; then\n",
    "        echo \"Input file not found! First in pair.\"; echo ${FORWARD}\n",
    "    fi\n",
    "    if [ ! -f ${REVERSE} ]; then\n",
    "        echo \"Input file not found! Second in pair.\"; echo ${REVERSE}\n",
    "    fi\n",
    "\n",
    "    echo \"Now I am processing ${FORWARD} as first in a pair and ${REVERSE} as a second in a pair with database $KRAKEN2_DB.\"\n",
    "    \n",
    "    kraken2 --use-names --paired --threads $THREADS --gzip-compressed --classified-out $OUTPUT_DIR/qc/kraken2/reads/${FORWARD%R1*}.classified-out#.fastq --unclassified-out $OUTPUT_DIR/qc/kraken2/reads/${FORWARD%R1*}.unclassified-out#.fastq --output $OUTPUT_DIR/qc/kraken2/reads/${FORWARD%R1*}.kraken.txt --report $OUTPUT_DIR/qc/kraken2/reads/${FORWARD%R1*}.kraken.report.txt --db $KRAKEN2_DB $FORWARD $REVERSE\n",
    "\n",
    "    for output in $OUTPUT_DIR/qc/kraken2/reads/*classified*.fastq\n",
    "    do\n",
    "        gzip $output &\n",
    "    done \n",
    "done\n",
    "\n",
    "wait\n",
    "\n",
    "# Get only species lines\n",
    "for report in $OUTPUT_DIR/qc/kraken2/reads/*.kraken.report.txt\n",
    "do \n",
    "    grep -P 'unclassified|root|\\tS\\t' $report | sort -k1,1nr > ${report%.kraken.report.txt*}.S.kraken.report.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bacteria reference genome alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With host genome DNA cleaned data we can proceed to the alignment to the reference. Please note we apply few filterings already at this step, mainly the minimal mapping quality, pairing of the reads and read duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) [bwa_index] Pack FASTA... 0.01 sec\n",
      "[bwa_index] Construct BWT for the packed sequence...\n",
      "[bwa_index] 0.25 seconds elapse.\n",
      "[bwa_index] Update BWT... 0.01 sec\n",
      "[bwa_index] Pack forward-only FASTA... 0.01 sec\n",
      "[bwa_index] Construct SA from BWT and Occ... 0.12 sec\n",
      "[main] Version: 0.7.15-r1140\n",
      "[main] CMD: bwa index /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa\n",
      "[main] Real time: 0.399 sec; CPU: 0.396 sec\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Now I am processing CW56_S1_R1.clean.fastq.gz as first in a pair and CW56_S1_R2.clean.fastq.gz as a second in a pair with reference /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (83, 132, 235)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 539)\n",
      "[M::mem_pestat] mean and std.dev: (168.32, 124.55)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 691)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (166, 234, 335)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 673)\n",
      "[M::mem_pestat] mean and std.dev: (257.99, 128.35)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 842)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (52, 100, 277)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 727)\n",
      "[M::mem_pestat] mean and std.dev: (113.93, 89.19)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 952)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (68, 184, 272)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 680)\n",
      "[M::mem_pestat] mean and std.dev: (186.00, 113.58)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 884)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (168, 237, 340)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 684)\n",
      "[M::mem_pestat] mean and std.dev: (261.61, 129.83)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 856)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (54, 104, 203)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 501)\n",
      "[M::mem_pestat] mean and std.dev: (114.17, 97.68)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 650)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (75, 146, 224)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 522)\n",
      "[M::mem_pestat] mean and std.dev: (163.36, 93.22)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 671)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (167, 237, 341)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 689)\n",
      "[M::mem_pestat] mean and std.dev: (261.79, 131.00)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 863)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (52, 73, 246)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 634)\n",
      "[M::mem_pestat] mean and std.dev: (125.24, 99.03)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 828)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (88, 152, 235)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 529)\n",
      "[M::mem_pestat] mean and std.dev: (168.62, 111.45)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 676)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (167, 236, 340)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 686)\n",
      "[M::mem_pestat] mean and std.dev: (261.27, 131.10)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 859)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (78, 186, 424)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 1116)\n",
      "[M::mem_pestat] mean and std.dev: (260.95, 255.05)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 1462)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (76, 123, 193)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 427)\n",
      "[M::mem_pestat] mean and std.dev: (121.47, 66.53)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 544)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (167, 237, 338)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 680)\n",
      "[M::mem_pestat] mean and std.dev: (260.75, 130.33)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 851)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (59, 100, 252)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 638)\n",
      "[M::mem_pestat] mean and std.dev: (116.82, 82.52)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 831)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (123, 160, 261)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 537)\n",
      "[M::mem_pestat] mean and std.dev: (170.52, 69.39)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 675)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (169, 237, 339)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 679)\n",
      "[M::mem_pestat] mean and std.dev: (261.64, 128.86)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 849)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (56, 104, 364)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 980)\n",
      "[M::mem_pestat] mean and std.dev: (185.73, 211.36)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 1288)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (68, 140, 287)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 725)\n",
      "[M::mem_pestat] mean and std.dev: (178.38, 134.36)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 944)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (166, 234, 333)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 667)\n",
      "[M::mem_pestat] mean and std.dev: (256.88, 126.64)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 834)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (80, 175, 309)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 767)\n",
      "[M::mem_pestat] mean and std.dev: (155.17, 101.11)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 996)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (49, 119, 184)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 454)\n",
      "[M::mem_pestat] mean and std.dev: (125.05, 104.55)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 589)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (164, 232, 330)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 662)\n",
      "[M::mem_pestat] mean and std.dev: (254.62, 125.54)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 828)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (80, 104, 277)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 671)\n",
      "[M::mem_pestat] mean and std.dev: (115.71, 71.29)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 868)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (59, 118, 161)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 365)\n",
      "[M::mem_pestat] mean and std.dev: (106.88, 60.97)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 467)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (164, 232, 329)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 659)\n",
      "[M::mem_pestat] mean and std.dev: (253.99, 125.16)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 824)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (85, 104, 288)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 694)\n",
      "[M::mem_pestat] mean and std.dev: (144.52, 98.83)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 897)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (90, 163, 266)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 618)\n",
      "[M::mem_pestat] mean and std.dev: (187.00, 122.20)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 794)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (164, 232, 330)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 662)\n",
      "[M::mem_pestat] mean and std.dev: (254.19, 125.91)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 828)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (72, 120, 252)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 612)\n",
      "[M::mem_pestat] mean and std.dev: (146.19, 112.39)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 792)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (72, 183, 291)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 729)\n",
      "[M::mem_pestat] mean and std.dev: (199.52, 164.44)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 948)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (165, 233, 330)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 660)\n",
      "[M::mem_pestat] mean and std.dev: (255.10, 125.32)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 825)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (84, 194, 892)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2508)\n",
      "[M::mem_pestat] mean and std.dev: (307.31, 371.91)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3316)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FF...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (94, 173, 260)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 592)\n",
      "[M::mem_pestat] mean and std.dev: (187.25, 114.43)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 758)\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation FR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (172, 241, 343)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 685)\n",
      "[M::mem_pestat] mean and std.dev: (265.37, 129.88)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 856)\n",
      "[M::mem_pestat] skip orientation RF as there are not enough pairs\n",
      "[M::mem_pestat] analyzing insert size distribution for orientation RR...\n",
      "[M::mem_pestat] (25, 50, 75) percentile: (78, 100, 995)\n",
      "[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 2829)\n",
      "[M::mem_pestat] mean and std.dev: (338.50, 467.50)\n",
      "[M::mem_pestat] low and high boundaries for proper pairs: (1, 3746)\n",
      "[M::mem_pestat] skip orientation FF\n",
      "[M::mem_pestat] skip orientation RR\n",
      "[main] Version: 0.7.15-r1140\n",
      "[main] CMD: bwa mem -t 12 -T 20 -v 1 -M -R @RG\\tID:1\\tLB:CW56_S1_R1\\tPL:Illumina\\tSM:CW56_S1_R1\\tPU:CW56_S1_R1 /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa CW56_S1_R1.clean.fastq.gz CW56_S1_R2.clean.fastq.gz\n",
      "[main] Real time: 64.832 sec; CPU: 672.536 sec\n",
      "Mapping finished\n",
      "Starting post-alignment processing and basic filtering\n",
      "[1] 34474\n",
      "[1]+  Done                    samtools flagstat $i > $OUTPUT_DIR/qc/alignment_stats/${i%.*}.flagstat\n",
      "Filtering finished\n",
      "Starting indel realignment\n",
      "[Wed Jun 26 20:01:18 CEST 2019] picard.sam.CreateSequenceDictionary REFERENCE=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa OUTPUT=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.dict    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 26 20:01:18 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "[Wed Jun 26 20:01:18 CEST 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.00 minutes.\n",
      "Runtime.totalMemory()=514850816\n",
      "INFO  20:01:20,172 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:20,175 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n",
      "INFO  20:01:20,175 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute \n",
      "INFO  20:01:20,175 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk \n",
      "INFO  20:01:20,176 HelpFormatter - [Wed Jun 26 20:01:20 CEST 2019] Executing on Linux 4.4.0-148-generic amd64 \n",
      "INFO  20:01:20,176 HelpFormatter - OpenJDK 64-Bit Server VM 1.8.0_192-b01 \n",
      "INFO  20:01:20,179 HelpFormatter - Program Args: -T RealignerTargetCreator --num_threads 12 -R /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa -I CW56_S1_.SS14.filt.bam -o /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results//mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14/alignment/CW56_S1_.SS14.filt.forIndelRealigner.intervals \n",
      "INFO  20:01:20,187 HelpFormatter - Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01. \n",
      "INFO  20:01:20,188 HelpFormatter - Date/Time: 2019/06/26 20:01:20 \n",
      "INFO  20:01:20,188 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:20,188 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:20,193 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  20:01:20,293 GenomeAnalysisEngine - Downsampling Settings: Method: BY_SAMPLE, Target Coverage: 1000 \n",
      "INFO  20:01:20,301 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,325 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.02 \n",
      "INFO  20:01:20,349 MicroScheduler - Running the GATK in parallel mode with 12 total threads, 1 CPU thread(s) for each of 12 data thread(s), of 70 processors available on this machine \n",
      "INFO  20:01:20,480 GenomeAnalysisEngine - Preparing for traversal over 1 BAM files \n",
      "INFO  20:01:20,514 GenomeAnalysisEngine - Done preparing for traversal \n",
      "INFO  20:01:20,514 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] \n",
      "INFO  20:01:20,514 ProgressMeter -                 | processed |    time |    per 1M |           |   total | remaining \n",
      "INFO  20:01:20,515 ProgressMeter -        Location |     sites | elapsed |     sites | completed | runtime |   runtime \n",
      "INFO  20:01:20,522 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,526 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,527 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,529 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,530 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,532 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,533 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,535 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,536 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,538 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,539 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,542 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,542 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,545 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,545 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,548 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,549 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,551 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,551 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,555 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:20,556 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:20,559 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.00 \n",
      "INFO  20:01:35,196 ProgressMeter -            done   1139569.0    14.0 s      12.0 s      100.0%    14.0 s       0.0 s \n",
      "INFO  20:01:35,197 ProgressMeter - Total runtime 14.68 secs, 0.24 min, 0.00 hours \n",
      "INFO  20:01:35,198 MicroScheduler - 68773 reads were filtered out during the traversal out of approximately 3985412 total reads (1.73%) \n",
      "INFO  20:01:35,198 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter \n",
      "INFO  20:01:35,198 MicroScheduler -   -> 0 reads (0.00% of total) failing BadMateFilter \n",
      "INFO  20:01:35,199 MicroScheduler -   -> 0 reads (0.00% of total) failing DuplicateReadFilter \n",
      "INFO  20:01:35,199 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter \n",
      "INFO  20:01:35,199 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter \n",
      "INFO  20:01:35,200 MicroScheduler -   -> 0 reads (0.00% of total) failing MappingQualityUnavailableFilter \n",
      "INFO  20:01:35,200 MicroScheduler -   -> 68773 reads (1.73% of total) failing MappingQualityZeroFilter \n",
      "INFO  20:01:35,200 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter \n",
      "INFO  20:01:35,200 MicroScheduler -   -> 0 reads (0.00% of total) failing Platform454Filter \n",
      "INFO  20:01:35,201 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter \n",
      "------------------------------------------------------------------------------------------\n",
      "Done. There were no warn messages.\n",
      "------------------------------------------------------------------------------------------\n",
      "INFO  20:01:37,046 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:37,049 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n",
      "INFO  20:01:37,049 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute \n",
      "INFO  20:01:37,049 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk \n",
      "INFO  20:01:37,049 HelpFormatter - [Wed Jun 26 20:01:37 CEST 2019] Executing on Linux 4.4.0-148-generic amd64 \n",
      "INFO  20:01:37,050 HelpFormatter - OpenJDK 64-Bit Server VM 1.8.0_192-b01 \n",
      "INFO  20:01:37,053 HelpFormatter - Program Args: -I CW56_S1_.SS14.filt.bam -R /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa -T IndelRealigner -LOD 2.5 --consensusDeterminationModel USE_SW -targetIntervals /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results//mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14/alignment/CW56_S1_.SS14.filt.forIndelRealigner.intervals -o /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results//mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14/alignment/CW56_S1_.SS14.filt.indelRealigned.bam \n",
      "INFO  20:01:37,062 HelpFormatter - Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01. \n",
      "INFO  20:01:37,062 HelpFormatter - Date/Time: 2019/06/26 20:01:37 \n",
      "INFO  20:01:37,062 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:37,062 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  20:01:37,146 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  20:01:37,228 GenomeAnalysisEngine - Downsampling Settings: No downsampling \n",
      "INFO  20:01:37,235 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  20:01:37,257 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.02 \n",
      "INFO  20:01:37,405 GenomeAnalysisEngine - Preparing for traversal over 1 BAM files \n",
      "INFO  20:01:37,410 GenomeAnalysisEngine - Done preparing for traversal \n",
      "INFO  20:01:37,410 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] \n",
      "INFO  20:01:37,411 ProgressMeter -                 | processed |    time |    per 1M |           |   total | remaining \n",
      "INFO  20:01:37,411 ProgressMeter -        Location |     reads | elapsed |     reads | completed | runtime |   runtime \n",
      "INFO  20:01:37,441 ReadShardBalancer$1 - Loading BAM index data \n",
      "INFO  20:01:37,442 ReadShardBalancer$1 - Done loading BAM index data \n",
      "INFO  20:02:07,438 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:47339    100002.0    30.0 s       5.0 m        4.2%    12.0 m      11.5 m \n",
      "INFO  20:02:37,442 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:99854    300040.0    60.0 s       3.3 m        8.8%    11.4 m      10.4 m \n",
      "INFO  20:03:08,044 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:154105    500052.0    90.0 s       3.0 m       13.5%    11.1 m       9.6 m \n",
      "INFO  20:03:38,047 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:208584    700069.0   120.0 s       2.9 m       18.3%    10.9 m       8.9 m \n"
     ]
    }
   ],
   "source": [
    "# Prepare reference indexes\n",
    "if [[ ${REFERENCE##*.} == \"gz\" ]] # Uncompress the reference if it is in gz archive\n",
    "then\n",
    "    gunzip -c $REFERENCE > ${REFERENCE%.gz*}\n",
    "    REFERENCE=${REFERENCE%.gz*}\n",
    "fi\n",
    "\n",
    "if [[ `echo $REFERENCE | grep \".fna$\"` != \"\" ]] || [[ `echo $REFERENCE | grep \".fasta$\"` != \"\" ]] # Make a link to the original to have \".fa\" suffix\n",
    "then\n",
    "    ln -s $REFERENCE ${REFERENCE%.*}.fa\n",
    "fi\n",
    "\n",
    "bwa index $REFERENCE\n",
    "samtools faidx $REFERENCE\n",
    "\n",
    "cd $OUTPUT_DIR/data/preprocessed\n",
    "\n",
    "OUTPUT_DIR=${OUTPUT_DIR}/$(basename $REFERENCE .fa} # Adjust output directory to reflect the used reference genome\n",
    "\n",
    "mkdir -p $OUTPUT_DIR/alignment\n",
    "mkdir -p $OUTPUT_DIR/qc/alignment_stats\n",
    "\n",
    "for sample in *R1*clean.fastq.gz\n",
    "do\n",
    "    FORWARD=$sample\n",
    "    extension=\"${FORWARD##*R1}\"\n",
    "    REVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "    # Input file check\n",
    "    if [ ! -f ${FORWARD} ]; then\n",
    "        echo \"Input file not found! First in pair.\"; echo ${FORWARD}\n",
    "    fi\n",
    "    if [ ! -f ${REVERSE} ]; then\n",
    "        echo \"Input file not found! Second in pair.\"; echo ${REVERSE}\n",
    "    fi\n",
    "\n",
    "    # Start mapping\n",
    "    echo \"Now I am processing ${FORWARD} as first in a pair and ${REVERSE} as a second in a pair with reference $REFERENCE\"\n",
    "\n",
    "    bwa mem -t $THREADS -T 20 -v 1 -M -R \"@RG\\tID:1\\tLB:${sample%%.*}\\tPL:Illumina\\tSM:${sample%%.*}\\tPU:${sample%%.*}\" $REFERENCE ${FORWARD} ${REVERSE} | samtools view -F 4 -@ $THREADS -b - | samtools sort -@ $THREADS - > $OUTPUT_DIR/alignment/${FORWARD%R1*}.$(basename $REFERENCE .fa).bam \n",
    "\n",
    "    echo \"Mapping finished\"\n",
    "\n",
    "    echo \"Starting post-alignment processing and basic filtering\"\n",
    "    \n",
    "    cd $OUTPUT_DIR/alignment\n",
    "\n",
    "    i=${FORWARD%R1*}.$(basename $REFERENCE .fa).bam\n",
    "\n",
    "    samtools index -@ $THREADS $i # Index BAM files\n",
    "    samtools flagstat $i > $OUTPUT_DIR/qc/alignment_stats/${i%.*}.flagstat &\n",
    "    samtools view -@ $THREADS -h -F 12 -f 2 -F 256 -b $i | samtools sort -n -@ $THREADS - | samtools fixmate -O bam - - | samtools sort -@ $THREADS - > ${i%.*}.filt.bam # -F 2048 = supplementary alignment, \"chimeric/non-linear alignments\"; -q $MAPQ\n",
    "    samtools index -@ $THREADS ${i%.*}.filt.bam\n",
    "    samtools flagstat ${i%.*}.filt.bam > $OUTPUT_DIR/qc/alignment_stats/${i%.*}.filt.flagstat\n",
    "\n",
    "    echo \"Filtering finished\"\n",
    "\n",
    "    echo \"Starting indel realignment\"\n",
    "    # Indel realignment\n",
    "    # Create input seqence dictionary and index reference\n",
    "    rm ${REFERENCE%.*}.dict\n",
    "    picard CreateSequenceDictionary R=$REFERENCE O=${REFERENCE%.*}.dict\n",
    "\n",
    "    # Realign\n",
    "    i=${i%.*}.filt.bam\n",
    "\n",
    "    gatk -T RealignerTargetCreator --num_threads $THREADS -R $REFERENCE -I ${i} -o $OUTPUT_DIR/alignment/${i%.*}.forIndelRealigner.intervals # Prepare intervals\n",
    "    gatk -I ${i} -R $REFERENCE -T IndelRealigner -LOD 2.5 --consensusDeterminationModel USE_SW -targetIntervals $OUTPUT_DIR/alignment/${i%.*}.forIndelRealigner.intervals -o $OUTPUT_DIR/alignment/${i%.*}.indelRealigned.bam # Run re-alignment\n",
    "\n",
    "    samtools index -@ $THREADS ${i%.*}.indelRealigned.bam\n",
    "\n",
    "    rm $i\n",
    "    rm $i*\n",
    "    rm $OUTPUT_DIR/alignment/${i%.*}.forIndelRealigner.intervals\n",
    "\n",
    "    echo \"Finished indel realignment\"\n",
    "\n",
    "    echo \"Starting read duplicate removal\"\n",
    "\n",
    "    # Remove duplicates\n",
    "    i=${i%.*}.indelRealigned.bam\n",
    "\n",
    "    mkdir $OUTPUT_DIR/qc/picard_dup\n",
    "    \n",
    "    picard MarkDuplicates INPUT=$i OUTPUT=${i%.*}.dedup.bam METRICS_FILE=$OUTPUT_DIR/qc/picard_dup/${i%.*}.dedupStats.txt REMOVE_DUPLICATES=true OPTICAL_DUPLICATE_PIXEL_DISTANCE=100 # Taggs ALL duplicates, PCR and optical and remove them\n",
    "\n",
    "    samtools index -@ $THREADS ${i%.*}.dedup.bam\n",
    "    samtools flagstat ${i%.*}.dedup.bam > $OUTPUT_DIR/alignment_stats/${i%.*}.dedup.flagstat\n",
    "\n",
    "    rm $i\n",
    "    rm $i*\n",
    "    \n",
    "    echo \"Finished read duplicate removal\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Post-alignment filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The most important part of the processing is the **post-alignment filtering**. `BWA MEM` is known to be very sensitive but not very specific. Right now, we have a lot of alignments which in reality do not belong to our reference. This is due to several factors used in BWA MEM, such as minimal length of alignment (default: 19, can be adjusted by `-k [INT]`), allowance of extensive soft-clipping, allowed hard-clipping and allowing a lot of mismatches. \n",
    "\n",
    "The simplest way to see the alignment artifacts and cross-mappings is to check the reference alignment coverage and check highly uneven coverage peaks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We apply the following filters:\n",
    "\n",
    "1. To many mismatches\n",
    "    * max. 0.05% mismatches and\n",
    "    * max. 5 mismatches\n",
    "2. Very short alignments\n",
    "     * min 35 bp mapping (measured on the read, not the reference)\n",
    "3. Too much softclipped \n",
    "     * max 0.05% soft-clipped\n",
    "4. Supplementary/chimeric reads \n",
    "    * `samtools -F 2048` flag\n",
    "5. Too much hardclipped\n",
    "     * max 0.00% hard-clipped (no hard-clipping allowed)\n",
    "6. MAPQ 10 \n",
    "    * very often repetitive alignments\n",
    "7. MAPQ 40 \n",
    "    * get only high quality alignment\n",
    "8. Singletons \n",
    "    * remove singleton reads (not paired) after all the mapping filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) mkdir: cannot create directory ‘/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/tmp’: File exists\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) Processing sample CW56_S1_.SS14.filt.indelRealigned.dedup.bam\n",
      "                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bDone! (0:24)\n",
      "851379 kept\n",
      "17931 failed\n",
      "                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bDone! (0:22)\n",
      "843691 kept\n",
      "7688 failed\n",
      "Number of too much mismatched reads is 23501 CW56_S1_.SS14.filt.indelRealigned.dedup.mm.fail.txt.tmp for sample  CW56_S1_.SS14.filt.indelRealigned.dedup.bam\n",
      "[Wed Jun 19 02:43:08 CEST 2019] picard.sam.FilterSamReads INPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.bam FILTER=includeReadList [OUTPUT SAM/BAM will contain reads that are supplied in the READ_LIST_FILE file] READ_LIST_FILE=CW56_S1_.SS14.filt.indelRealigned.dedup.mm.fail.txt.tmp OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.mm.filtOut.bam    WRITE_READS_FILES=true VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 02:43:08 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "INFO\t2019-06-19 02:43:12\tFilterSamReads\tFiltering [presorted=true] CW56_S1_.SS14.filt.indelRealigned.dedup.bam -> OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.mm.filtOut.bam [sortorder=coordinate]\n",
      "INFO\t2019-06-19 02:43:15\tFilterSamReads\t47,002 SAMRecords written to CW56_S1_.SS14.filt.indelRealigned.dedup.mm.filtOut.bam\n",
      "[Wed Jun 19 02:43:15 CEST 2019] picard.sam.FilterSamReads done. Elapsed time: 0.12 minutes.\n",
      "Runtime.totalMemory()=708837376\n",
      "[1] 2165\n",
      "[1]+  Done                    samtools view -h -@ $THREADS ${sample%.*}.mm2.bam | perl -slane '$l = 0; $F[5] =~ s/(\\d+)[MX=DN]/$l+=$1/eg; print if $l < $MIN_LENGTH_MAPPED or /^@/' -- -MIN_LENGTH_MAPPED=$MIN_LENGTH_MAPPED | samtools view -@ $THREADS -b - > ${sample%.*}.short.filtOut.bam\n",
      "                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bDone! (0:19)\n",
      "Wrote: 840236 reads\n",
      "Altered: 13046\n",
      "Unmapped: 0\n",
      "Number of too much soft clipped reads is 10155 CW56_S1_.SS14.filt.indelRealigned.dedup.read_names_to_remove_highSoftClip.txt for sample  CW56_S1_.SS14.filt.indelRealigned.dedup.bam\n",
      "[1] 2258\n",
      "[Wed Jun 19 02:43:48 CEST 2019] picard.sam.FilterSamReads INPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.short.bam FILTER=excludeReadList [OUTPUT bam will contain reads that are *not* supplied in the READ_LIST_FILE file] READ_LIST_FILE=CW56_S1_.SS14.filt.indelRealigned.dedup.read_names_to_remove_highSoftClip.txt OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.scf.bam    WRITE_READS_FILES=true VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 02:43:48 CEST 2019] picard.sam.FilterSamReads INPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.short.bam FILTER=includeReadList [OUTPUT SAM/BAM will contain reads that are supplied in the READ_LIST_FILE file] READ_LIST_FILE=CW56_S1_.SS14.filt.indelRealigned.dedup.read_names_to_remove_highSoftClip.txt OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.scf.filtOut.bam    WRITE_READS_FILES=true VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 02:43:48 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "[Wed Jun 19 02:43:48 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "INFO\t2019-06-19 02:43:52\tFilterSamReads\tFiltering [presorted=true] CW56_S1_.SS14.filt.indelRealigned.dedup.short.bam -> OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.scf.bam [sortorder=coordinate]\n",
      "INFO\t2019-06-19 02:43:52\tFilterSamReads\tFiltering [presorted=true] CW56_S1_.SS14.filt.indelRealigned.dedup.short.bam -> OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.scf.filtOut.bam [sortorder=coordinate]\n",
      "INFO\t2019-06-19 02:43:55\tFilterSamReads\t18,480 SAMRecords written to CW56_S1_.SS14.filt.indelRealigned.dedup.scf.filtOut.bam\n",
      "[Wed Jun 19 02:43:55 CEST 2019] picard.sam.FilterSamReads done. Elapsed time: 0.10 minutes.\n",
      "Runtime.totalMemory()=708313088\n",
      "INFO\t2019-06-19 02:44:00\tFilterSamReads\t821,756 SAMRecords written to CW56_S1_.SS14.filt.indelRealigned.dedup.scf.bam\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.scf.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.scf.bam\n",
      "[Wed Jun 19 02:44:03 CEST 2019] picard.sam.FilterSamReads done. Elapsed time: 0.24 minutes.\n",
      "Runtime.totalMemory()=709885952\n",
      "[1]+  Done                    picard FilterSamReads I=${sample%.*}.short.bam O=${sample%.*}.scf.filtOut.bam READ_LIST_FILE=${sample%.*}.read_names_to_remove_highSoftClip.txt FILTER=includeReadList\n",
      "[1] 2468\n",
      "[1]+  Done                    samtools view -@ $THREADS -h -f 2048 -b ${sample%.*}.scf.bam > ${sample%.*}.sup.filtOut.bam\n",
      "[1] 2500\n",
      "[1]+  Done                    samtools view -@ $THREADS -h ${sample%.*}.sup.bam | awk '$6 ~ /H/{print}' | samtools view -bh - > ${sample%.*}.hcf.filtOut.bam\n",
      "[1] 2532\n",
      "[2] 2563\n",
      "[1]-  Done                    samtools view -@ $THREADS -h ${sample%.*}.sup.bam | awk -v var=\"$MAPQ\" '$5 < var || $1 ~ /^@/' | samtools view -b - > ${sample%.*}.MAPQ${MAPQ}.filtOut.bam\n",
      "[2]+  Done                    samtools view -@ $THREADS -h ${sample%.*}.MAPQ${MAPQ}.bam | awk -v var=\"$MAPQ_FINAL\" '$5 < var || $1 ~ /^@/' | samtools view -b - > ${sample%.*}.MAPQ${MAPQ_FINAL}.filtOut.bam\n",
      "Number of singleton reads after filtering is 19963 CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.singleAfterFilt.txt for sample  CW56_S1_.SS14.filt.indelRealigned.dedup.bam\n",
      "[1] 2611\n",
      "[Wed Jun 19 02:44:32 CEST 2019] picard.sam.FilterSamReads INPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam FILTER=excludeReadList [OUTPUT bam will contain reads that are *not* supplied in the READ_LIST_FILE file] READ_LIST_FILE=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.singleAfterFilt.txt OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.tmp    WRITE_READS_FILES=true VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 02:44:32 CEST 2019] picard.sam.FilterSamReads INPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam FILTER=includeReadList [OUTPUT SAM/BAM will contain reads that are supplied in the READ_LIST_FILE file] READ_LIST_FILE=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.singleAfterFilt.txt OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.singletons.filtOut.bam    WRITE_READS_FILES=true VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 02:44:32 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "[Wed Jun 19 02:44:32 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "INFO\t2019-06-19 02:44:36\tFilterSamReads\tFiltering [presorted=true] CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam -> OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.tmp [sortorder=coordinate]\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "WARNING: BAM index file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.bai is older than BAM /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "INFO\t2019-06-19 02:44:36\tFilterSamReads\tFiltering [presorted=true] CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam -> OUTPUT=CW56_S1_.SS14.filt.indelRealigned.dedup.singletons.filtOut.bam [sortorder=coordinate]\n",
      "INFO\t2019-06-19 02:44:38\tFilterSamReads\t19,963 SAMRecords written to CW56_S1_.SS14.filt.indelRealigned.dedup.singletons.filtOut.bam\n",
      "[Wed Jun 19 02:44:38 CEST 2019] picard.sam.FilterSamReads done. Elapsed time: 0.11 minutes.\n",
      "Runtime.totalMemory()=708837376\n",
      "INFO\t2019-06-19 02:44:44\tFilterSamReads\t786,004 SAMRecords written to CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam.tmp\n",
      "[Wed Jun 19 02:44:46 CEST 2019] picard.sam.FilterSamReads done. Elapsed time: 0.24 minutes.\n",
      "Runtime.totalMemory()=709361664\n",
      "[1]+  Done                    picard FilterSamReads I=${sample%.*}.MAPQ${MAPQ_FINAL}.bam O=${sample%.*}.singletons.filtOut.bam READ_LIST_FILE=${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt FILTER=includeReadList\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Java memory size is set to 1200M\n",
      "Launching application...\n",
      "\n",
      "QualiMap v.2.2.2-dev\n",
      "Built on 2017-08-28 08:37\n",
      "\n",
      "Selected tool: bamqc\n",
      "Available memory (Mb): 32\n",
      "Max memory (Mb): 1118\n",
      "Wed Jun 19 02:45:53 CEST 2019\t\tWARNING\tOutput folder already exists, the results will be saved there\n",
      "\n",
      "Starting bam qc....\n",
      "Loading sam header...\n",
      "Loading locator...\n",
      "Loading reference...\n",
      "Number of windows: 400, effective number of windows: 400\n",
      "Chunk of reads size: 1000\n",
      "Number of threads: 12\n",
      "Processed 50 out of 400 windows...\n",
      "Processed 100 out of 400 windows...\n",
      "Processed 150 out of 400 windows...\n",
      "Processed 200 out of 400 windows...\n",
      "Processed 250 out of 400 windows...\n",
      "Processed 300 out of 400 windows...\n",
      "Processed 350 out of 400 windows...\n",
      "Processed 400 out of 400 windows...\n",
      "Total processed windows:400\n",
      "Number of reads: 869310\n",
      "Number of valid reads: 869310\n",
      "Number of correct strand reads:0\n",
      "\n",
      "Inside of regions...\n",
      "Num mapped reads: 869310\n",
      "Num mapped first of pair: 434655\n",
      "Num mapped second of pair: 434655\n",
      "Num singletons: 0\n",
      "Time taken to analyze reads: 8\n",
      "Computing descriptors...\n",
      "numberOfMappedBases: 116157204\n",
      "referenceSize: 1139569\n",
      "numberOfSequencedBases: 116151139\n",
      "numberOfAs: 27241454\n",
      "Computing per chromosome statistics...\n",
      "Computing histograms...\n",
      "Overall analysis time: 8\n",
      "end of bam qc\n",
      "Computing report...\n",
      "Writing PDF report...\n",
      "PDF file created successfully \n",
      "\n",
      "Finished\n",
      "Java memory size is set to 1200M\n",
      "Launching application...\n",
      "\n",
      "QualiMap v.2.2.2-dev\n",
      "Built on 2017-08-28 08:37\n",
      "\n",
      "Selected tool: bamqc\n",
      "Available memory (Mb): 32\n",
      "Max memory (Mb): 1118\n",
      "Wed Jun 19 02:46:04 CEST 2019\t\tWARNING\tOutput folder already exists, the results will be saved there\n",
      "\n",
      "Starting bam qc....\n",
      "Loading sam header...\n",
      "Loading locator...\n",
      "Loading reference...\n",
      "Number of windows: 400, effective number of windows: 400\n",
      "Chunk of reads size: 1000\n",
      "Number of threads: 12\n",
      "Processed 50 out of 400 windows...\n",
      "Processed 100 out of 400 windows...\n",
      "Processed 150 out of 400 windows...\n",
      "Processed 200 out of 400 windows...\n",
      "Processed 250 out of 400 windows...\n",
      "Processed 300 out of 400 windows...\n",
      "Processed 350 out of 400 windows...\n",
      "Processed 400 out of 400 windows...\n",
      "Total processed windows:400\n",
      "Number of reads: 869310\n",
      "Number of valid reads: 869310\n",
      "Number of correct strand reads:0\n",
      "\n",
      "Inside of regions...\n",
      "Num mapped reads: 869310\n",
      "Num mapped first of pair: 434655\n",
      "Num mapped second of pair: 434655\n",
      "Num singletons: 0\n",
      "Time taken to analyze reads: 8\n",
      "Computing descriptors...\n",
      "numberOfMappedBases: 116157204\n",
      "referenceSize: 1139569\n",
      "numberOfSequencedBases: 116151139\n",
      "numberOfAs: 27241454\n",
      "Computing per chromosome statistics...\n",
      "Computing histograms...\n",
      "Overall analysis time: 8\n",
      "end of bam qc\n",
      "Computing report...\n",
      "Writing HTML report...\n",
      "HTML report created successfully\n",
      "\n",
      "Finished\n",
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:45: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  configs = yaml.load(f)\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:51: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  sp = yaml.load(f)\n",
      "[INFO   ]         multiqc : This is MultiQC v1.7\n",
      "[INFO   ]         multiqc : Template    : default\n",
      "[INFO   ]         multiqc : Searching '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/qualimap/'\n",
      "\u001b[?25lSearching 52 files..  [####################################]  100%          \u001b[?25h\n",
      "[INFO   ]        qualimap : Found 1 BamQC reports\n",
      "[INFO   ]         multiqc : Compressing plot data\n",
      "[INFO   ]         multiqc : Report      : ../qc/qualimap/multiqc_report.html\n",
      "[INFO   ]         multiqc : Data        : ../qc/qualimap/multiqc_data\n",
      "[INFO   ]         multiqc : MultiQC complete\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) rm: cannot remove '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/*.bam.tmp': No such file or directory\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) mv: cannot move '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/filtered' to a subdirectory of itself, '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/filtered/filt/filtered'\n",
      "(treponema) (.env) mv: cannot stat '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/filt/*.MAPQ40.bam*': No such file or directory\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "MAX_PERC_OF_MM=0.05 # Maximum percentage of mismatches compared to the read length - bad if we have to error-prone reads but helps to remove false-positives\n",
    "MAX_NUMBER_OF_MM=5 # Maximum number of mismatches\n",
    "MIN_LENGTH_MAPPED=35 # Remove mappings that mapped with too few bases (remove excesive soft-clipping)\n",
    "MAX_SOFTCLIP=0.05 # Maximal percentage of the reads allowed to be soft-clipped\n",
    "MAX_HARDCLIP=0.00 # Maximal percentage of the reads allowed to be hard-clipped\n",
    "MAPQ=10 # Minimal MAPQ for (probably) repetitive regions\n",
    "MAPQ_FINAL=40 # Minimal MAPQ for final results\n",
    "\n",
    "cd $OUTPUT_DIR/alignment\n",
    "\n",
    "mkdir $OUTPUT_DIR/tmp\n",
    "\n",
    "# Filter mappings\n",
    "for sample in *.filt.indelRealigned.dedup.bam\n",
    "do \n",
    "    echo \"Processing sample $sample\"\n",
    "\n",
    "    samtools index -@ $THREADS $sample\n",
    "\n",
    "    # 1) Remove too many mismatches\n",
    "    bamutils filter $sample ${sample%.*}.mm1.bam -failed ${sample%.*}.mm1.fail.txt -maximum_mismatch_ratio $MAX_PERC_OF_MM # Filter by perc. of mismatches; It's more filtering on edit distance = how many nucleotides have to be changed to get exactly the reference sequence; indels are counted as many times as they are \"long\"; error in the source https://github.com/ngsutils/ngsutils/pull/18\n",
    "    bamutils filter ${sample%.*}.mm1.bam ${sample%.*}.mm2.bam -failed ${sample%.*}.mm2.fail.txt -mismatch $MAX_NUMBER_OF_MM # Filter by num. of mismatches\n",
    "\n",
    "    cat ${sample%.*}.mm1.fail.txt ${sample%.*}.mm2.fail.txt > ${sample%.*}.mm.fail.txt # Merge mapping filtered by mismatches\n",
    "\n",
    "    # Use extracted filtered read name and filter it out from the original bam but ONLY when the mismatches filtering had some results\n",
    "    if [ -s ${sample%.*}.mm.fail.txt ]\n",
    "    then\n",
    "        cat ${sample%.*}.mm.fail.txt | awk '{print $1}' | sort -T $OUTPUT_DIR/tmp --parallel=$THREADS | uniq > ${sample%.*}.mm.fail.txt.tmp\n",
    "        echo \"Number of too much mismatched reads is\" `wc -l ${sample%.*}.mm.fail.txt.tmp` \"for sample \" $sample\n",
    "        picard FilterSamReads I=$sample O=${sample%.*}.mm.filtOut.bam READ_LIST_FILE=${sample%.*}.mm.fail.txt.tmp FILTER=includeReadList # Include reads\n",
    "        rm ${sample%.*}.mm.fail.txt.tmp\n",
    "    else\n",
    "        echo \"There are none to much mismatched reads with \" $MAX_PERC_OF_MM \" % and \" $MAX_NUMBER_OF_MM \" mismatches for sample \" $sample\". Nothing to report.\"\n",
    "    fi\n",
    "\n",
    "    # 2) Remove too short alignment\n",
    "    # Filter out mappings that mapped with length shorter than MIN_LENGTH_MAPPED https://www.biostars.org/p/12406/ - this filters the mapping by calculating the length of mapping on the read itself; example 106S8M1D21M14S with MIN_LENGTH_MAPPED=30 is filtered out because 8M+21M=29\n",
    "    # Another filtering by mapped read length could be taken from here https://www.biostars.org/p/151510/ - this filters the mapping by calculating the length of mapping on the reference; example 106S8M1D21M14S with MIN_LENGTH_MAPPED=30 is NOT filtered out because 8M+1D+21M=30, it is filtered out when set to MIN_LENGTH_MAPPED=31\n",
    "    #samjs.jar -e '!record.readUnmappedFlag  && record.cigar.referenceLength  >= $MIN_LENGTH_MAPPED '$sample | samtools view -@ $THREADS -b -o ${sample%.*}.short.bam -\n",
    "    samtools view -h -@ $THREADS ${sample%.*}.mm2.bam | perl -slane '$l = 0; $F[5] =~ s/(\\d+)[MX=DN]/$l+=$1/eg; print if $l < $MIN_LENGTH_MAPPED or /^@/' -- -MIN_LENGTH_MAPPED=$MIN_LENGTH_MAPPED | samtools view -@ $THREADS -b - > ${sample%.*}.short.filtOut.bam &\n",
    "    samtools view -h -@ $THREADS ${sample%.*}.mm2.bam | perl -slane '$l = 0; $F[5] =~ s/(\\d+)[MX=DN]/$l+=$1/eg; print if $l >= $MIN_LENGTH_MAPPED or /^@/' -- -MIN_LENGTH_MAPPED=$MIN_LENGTH_MAPPED | samtools view -@ $THREADS -b - > ${sample%.*}.short.bam\n",
    "\n",
    "    # 3) Too much softclipped\n",
    "    # Running it on the whole file and saving it creates error in BAM validation; we have to just take the read names from here and remove them from the alignment\n",
    "    bamutils removeclipping ${sample%.*}.short.bam ${sample%.*}.scf.bam.tmp\n",
    "    samtools view -@ $THREADS ${sample%.*}.scf.bam.tmp | grep 'ZC:f:' | awk '{for (i=1;i<=NF;i++){if ($sample ~/ZC:f:/) {print $1, $sample}}}' | sed 's/ZC:f://' | awk -v MAX_SOFTCLIP=\"$MAX_SOFTCLIP\" ' $2 > MAX_SOFTCLIP {print $1}' > ${sample%.*}.read_names_to_remove_highSoftClip.txt.tmp # Get only softclipped reads above the threshold\n",
    "    rm ${sample%.*}.scf.bam.tmp # Remove temporary BAMs\n",
    "    cat ${sample%.*}.read_names_to_remove_highSoftClip.txt.tmp | cut -f 1 | sort -T $OUTPUT_DIR/tmp --parallel=$THREADS | uniq > ${sample%.*}.read_names_to_remove_highSoftClip.txt # Get unique read names with softclipping!\n",
    "    #cat ${sample%.*}.read_names_to_remove_highSoftClip.txt | cut -f 1 | sort -T $OUTPUT_DIR/tmp --parallel=$THREADS | uniq -d > tmp; mv tmp ${sample%.*}.read_names_to_remove_highSoftClip.txt # Get unique read names with softclipping if whole pair failed the filtering!\n",
    "    rm ${sample%.*}.read_names_to_remove_highSoftClip.txt.tmp\n",
    "\n",
    "    # Use extracted filtered read name and filter it out from the original bam but ONLY when the soft clipping filtering had some results\n",
    "    if [ -s ${sample%.*}.read_names_to_remove_highSoftClip.txt ]\n",
    "    then\n",
    "        echo \"Number of too much soft clipped reads is\" `wc -l ${sample%.*}.read_names_to_remove_highSoftClip.txt` \"for sample \" $sample\n",
    "        picard FilterSamReads I=${sample%.*}.short.bam O=${sample%.*}.scf.filtOut.bam READ_LIST_FILE=${sample%.*}.read_names_to_remove_highSoftClip.txt FILTER=includeReadList & # Include reads\n",
    "        picard FilterSamReads I=${sample%.*}.short.bam O=${sample%.*}.scf.bam READ_LIST_FILE=${sample%.*}.read_names_to_remove_highSoftClip.txt FILTER=excludeReadList # Exclude reads\n",
    "    else\n",
    "        echo \"There are none to much soft clipped reads with \" $MAX_SOFTCLIP \" % soft clipping for sample \" $sample. Continue without filtering.\n",
    "        mv ${sample%.*}.short.bam ${sample%.*}.scf.bam\n",
    "    fi\n",
    "\n",
    "    samtools index -@ $THREADS ${sample%.*}.scf.bam\n",
    "\n",
    "    # 4) Supplementary/chimeric alignment\n",
    "    samtools view -@ $THREADS -h -f 2048 -b ${sample%.*}.scf.bam > ${sample%.*}.sup.filtOut.bam & # get -F 2048 \"chimeric/non-linear alignments\" - might be interesting for translocations; possible overlap with hardclipping \n",
    "    samtools view -@ $THREADS -h -F 2048 -b ${sample%.*}.scf.bam > ${sample%.*}.sup.bam # remove -F 2048 \"chimeric/non-linear alignments\" - might be interesting for translocations; possible overlap with hardclipping\n",
    "\n",
    "    # 5) Hardclipped\n",
    "    samtools view -@ $THREADS -h ${sample%.*}.sup.bam | awk '$6 ~ /H/{print}' | samtools view -bh - > ${sample%.*}.hcf.filtOut.bam & # Get only hardclipped mappings; hardclipped alignments might mean the part of the alignment that is hardclipped might map to different part of the genome - possible chimeric reads? https://www.biostars.org/p/109333/\n",
    "    samtools view -@ $THREADS -h ${sample%.*}.sup.bam | awk '$6 !~ /H/{print}' | samtools view -bh - > ${sample%.*}.hcf.bam # https://www.biostars.org/p/137461/\n",
    "\n",
    "    # 6) MAPQ10 – very often repetitive alignments\n",
    "    samtools view -@ $THREADS -h ${sample%.*}.sup.bam | awk -v var=\"$MAPQ\" '$5 < var || $1 ~ /^@/' | samtools view -b - > ${sample%.*}.MAPQ${MAPQ}.filtOut.bam &\n",
    "    samtools view -@ $THREADS -h -bq $MAPQ ${sample%.*}.sup.bam > ${sample%.*}.MAPQ${MAPQ}.bam # BWA-MEM -T settings should filter all low MAPQ mappings; -F 2048 \"chimeric/non-linear alignments\" - might be interesting for translocations\n",
    "\n",
    "    # 7) MAPQ40 – Only high quality alignments\n",
    "    samtools view -@ $THREADS -h ${sample%.*}.MAPQ${MAPQ}.bam | awk -v var=\"$MAPQ_FINAL\" '$5 < var || $1 ~ /^@/' | samtools view -b - > ${sample%.*}.MAPQ${MAPQ_FINAL}.filtOut.bam &\n",
    "    samtools view -@ $THREADS -h -bq $MAPQ_FINAL ${sample%.*}.MAPQ${MAPQ}.bam > ${sample%.*}.MAPQ${MAPQ_FINAL}.bam # Get only very high quality mappings\n",
    "\n",
    "    # 8) Singletons\n",
    "    # Remove reads that remained as singletons after all filtering steps - works only for paired-end sequencing!\n",
    "    samtools view -@ $THREADS ${sample%.*}.MAPQ${MAPQ_FINAL}.bam | cut -f 1 | sort -T $OUTPUT_DIR/tmp --parallel=$THREADS | uniq -u > ${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt\n",
    "\n",
    "    if [ -s ${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt ]\n",
    "    then\n",
    "        echo \"Number of singleton reads after filtering is\" `wc -l ${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt` \"for sample \" $sample\n",
    "        picard FilterSamReads I=${sample%.*}.MAPQ${MAPQ_FINAL}.bam O=${sample%.*}.singletons.filtOut.bam READ_LIST_FILE=${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt FILTER=includeReadList & # Include reads\n",
    "        picard FilterSamReads I=${sample%.*}.MAPQ${MAPQ_FINAL}.bam O=${sample%.*}.MAPQ${MAPQ_FINAL}.bam.tmp READ_LIST_FILE=${sample%.*}.MAPQ${MAPQ_FINAL}.singleAfterFilt.txt FILTER=excludeReadList # Exclude reads\n",
    "        mv ${sample%.*}.MAPQ${MAPQ_FINAL}.bam.tmp ${sample%.*}.MAPQ${MAPQ_FINAL}.bam\n",
    "    else\n",
    "        echo \"There are none singleton reads after filtering for sample \" $sample. Continue without filtering.\n",
    "    fi\n",
    "\n",
    "    samtools index -@ $THREADS ${sample%.*}.MAPQ${MAPQ_FINAL}.bam\n",
    "\n",
    "    sleep 60 # wait/sleep for one minute for the jobs to finish (if they didn't)\n",
    "\n",
    "done\n",
    "\n",
    "for sample in $OUTPUT_DIR/alignment/*.bam\n",
    "do\n",
    "    samtools index -@ $THREADS $sample\n",
    "done\n",
    "\n",
    "# Do some cleaning\n",
    "rm $OUTPUT_DIR/alignment/*.bam.tmp $OUTPUT_DIR/alignment/*.read_names_to_remove_highSoftClip.txt $OUTPUT_DIR/alignment/*.reads $OUTPUT_DIR/alignment/*.singleAfterFilt.txt $OUTPUT_DIR/alignment/*.fail.txt\n",
    "\n",
    "mkdir -p $OUTPUT_DIR/alignment/filtered/filtOut\n",
    "mkdir -p $OUTPUT_DIR/alignment/filtered/filt\n",
    "\n",
    "mv $OUTPUT_DIR/alignment/*.filtOut.bam* $OUTPUT_DIR/alignment/filtered/filtOut/\n",
    "mv $OUTPUT_DIR/alignment/* $OUTPUT_DIR/alignment/filtered/filt/\n",
    "mv $OUTPUT_DIR/alignment/filtered/filt/*.MAPQ40.bam* $OUTPUT_DIR/alignment/filtered/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Please note that the filtering applied is rather strict. We might lose some true positive mappings but in this case we rather focus on strongly supported alignments to be sure we are detecting what we want to detect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alignment statistics and reference genome coverage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One of the the quality checks after the filtering and before all the other steps is the genome coverage statistics...and of course general mapping statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Java memory size is set to 1200M\n",
      "Launching application...\n",
      "\n",
      "QualiMap v.2.2.2-dev\n",
      "Built on 2017-08-28 08:37\n",
      "\n",
      "Selected tool: bamqc\n",
      "Available memory (Mb): 32\n",
      "Max memory (Mb): 1118\n",
      "Wed Jun 19 03:22:48 CEST 2019\t\tWARNING\tOutput folder already exists, the results will be saved there\n",
      "\n",
      "Starting bam qc....\n",
      "Loading sam header...\n",
      "Loading locator...\n",
      "Loading reference...\n",
      "Number of windows: 400, effective number of windows: 400\n",
      "Chunk of reads size: 1000\n",
      "Number of threads: 12\n",
      "Processed 50 out of 400 windows...\n",
      "Processed 100 out of 400 windows...\n",
      "Processed 150 out of 400 windows...\n",
      "Processed 200 out of 400 windows...\n",
      "Processed 250 out of 400 windows...\n",
      "Processed 300 out of 400 windows...\n",
      "Processed 350 out of 400 windows...\n",
      "Processed 400 out of 400 windows...\n",
      "Total processed windows:400\n",
      "Number of reads: 786004\n",
      "Number of valid reads: 786004\n",
      "Number of correct strand reads:0\n",
      "\n",
      "Inside of regions...\n",
      "Num mapped reads: 786004\n",
      "Num mapped first of pair: 393002\n",
      "Num mapped second of pair: 393002\n",
      "Num singletons: 0\n",
      "Time taken to analyze reads: 8\n",
      "Computing descriptors...\n",
      "numberOfMappedBases: 105562664\n",
      "referenceSize: 1139569\n",
      "numberOfSequencedBases: 105559115\n",
      "numberOfAs: 24800265\n",
      "Computing per chromosome statistics...\n",
      "Computing histograms...\n",
      "Overall analysis time: 8\n",
      "end of bam qc\n",
      "Computing report...\n",
      "Writing PDF report...\n",
      "PDF file created successfully \n",
      "\n",
      "Finished\n",
      "Java memory size is set to 1200M\n",
      "Launching application...\n",
      "\n",
      "QualiMap v.2.2.2-dev\n",
      "Built on 2017-08-28 08:37\n",
      "\n",
      "Selected tool: bamqc\n",
      "Available memory (Mb): 32\n",
      "Max memory (Mb): 1118\n",
      "Wed Jun 19 03:22:58 CEST 2019\t\tWARNING\tOutput folder already exists, the results will be saved there\n",
      "\n",
      "Starting bam qc....\n",
      "Loading sam header...\n",
      "Loading locator...\n",
      "Loading reference...\n",
      "Number of windows: 400, effective number of windows: 400\n",
      "Chunk of reads size: 1000\n",
      "Number of threads: 12\n",
      "Processed 50 out of 400 windows...\n",
      "Processed 100 out of 400 windows...\n",
      "Processed 150 out of 400 windows...\n",
      "Processed 200 out of 400 windows...\n",
      "Processed 250 out of 400 windows...\n",
      "Processed 300 out of 400 windows...\n",
      "Processed 350 out of 400 windows...\n",
      "Processed 400 out of 400 windows...\n",
      "Total processed windows:400\n",
      "Number of reads: 786004\n",
      "Number of valid reads: 786004\n",
      "Number of correct strand reads:0\n",
      "\n",
      "Inside of regions...\n",
      "Num mapped reads: 786004\n",
      "Num mapped first of pair: 393002\n",
      "Num mapped second of pair: 393002\n",
      "Num singletons: 0\n",
      "Time taken to analyze reads: 8\n",
      "Computing descriptors...\n",
      "numberOfMappedBases: 105562664\n",
      "referenceSize: 1139569\n",
      "numberOfSequencedBases: 105559115\n",
      "numberOfAs: 24800265\n",
      "Computing per chromosome statistics...\n",
      "Computing histograms...\n",
      "Overall analysis time: 8\n",
      "end of bam qc\n",
      "Computing report...\n",
      "Writing HTML report...\n",
      "HTML report created successfully\n",
      "\n",
      "Finished\n",
      "(treponema) (.env) (treponema) (.env) /mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:45: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  configs = yaml.load(f)\n",
      "/mnt/nfs/home/323639/000000-My_Documents/VM-home/tools/miniconda2/envs/treponema/lib/python2.7/site-packages/multiqc/utils/config.py:51: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  sp = yaml.load(f)\n",
      "[INFO   ]         multiqc : This is MultiQC v1.7\n",
      "[INFO   ]         multiqc : Template    : default\n",
      "[INFO   ]         multiqc : Searching '/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/qualimap/'\n",
      "\u001b[?25lSearching 50 files..  [####################################]  100%          \u001b[?25h\n",
      "[INFO   ]        qualimap : Found 1 BamQC reports\n",
      "[INFO   ]         multiqc : Compressing plot data\n",
      "[INFO   ]         multiqc : Report      : ../../qc/qualimap/multiqc_report.html\n",
      "[INFO   ]         multiqc : Data        : ../../qc/qualimap/multiqc_data\n",
      "[INFO   ]         multiqc : MultiQC complete\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) [Wed Jun 19 03:23:17 CEST 2019] picard.sam.CreateSequenceDictionary REFERENCE=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa OUTPUT=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.dict    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Wed Jun 19 03:23:17 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "[Wed Jun 19 03:23:17 CEST 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.00 minutes.\n",
      "Runtime.totalMemory()=514850816\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Going to process files CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "(treponema) (.env) (treponema) (.env) INFO  03:23:20,262 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  03:23:20,264 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n",
      "INFO  03:23:20,264 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute \n",
      "INFO  03:23:20,265 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk \n",
      "INFO  03:23:20,265 HelpFormatter - [Wed Jun 19 03:23:20 CEST 2019] Executing on Linux 4.4.0-148-generic amd64 \n",
      "INFO  03:23:20,265 HelpFormatter - OpenJDK 64-Bit Server VM 1.8.0_192-b01 \n",
      "INFO  03:23:20,268 HelpFormatter - Program Args: -T DepthOfCoverage -R /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa -I /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/alignment/filtered/input_bams.list -o /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/qc/coverage/SS14.coverage -ct 3 -ct 5 -ct 10 \n",
      "INFO  03:23:20,277 HelpFormatter - Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01. \n",
      "INFO  03:23:20,277 HelpFormatter - Date/Time: 2019/06/19 03:23:20 \n",
      "INFO  03:23:20,277 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  03:23:20,277 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  03:23:20,287 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  03:23:20,381 GenomeAnalysisEngine - Downsampling Settings: No downsampling \n",
      "INFO  03:23:20,389 SAMDataSource$SAMReaders - Initializing SAMRecords in serial \n",
      "INFO  03:23:20,413 SAMDataSource$SAMReaders - Done initializing BAM readers: total time 0.02 \n",
      "INFO  03:23:20,602 GenomeAnalysisEngine - Preparing for traversal over 1 BAM files \n",
      "INFO  03:23:20,640 GenomeAnalysisEngine - Done preparing for traversal \n",
      "INFO  03:23:20,640 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] \n",
      "INFO  03:23:20,641 ProgressMeter -                 | processed |    time |    per 1M |           |   total | remaining \n",
      "INFO  03:23:20,641 ProgressMeter -        Location |     sites | elapsed |     sites | completed | runtime |   runtime \n",
      "INFO  03:23:50,648 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:354865    344064.0    30.0 s      87.0 s       31.1%    96.0 s      66.0 s \n",
      "INFO  03:24:20,651 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:745681    737280.0    60.0 s      81.0 s       65.4%    91.0 s      31.0 s \n",
      "INFO  03:24:50,654 ProgressMeter - gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome:1097729   1097728.0    90.0 s      81.0 s       96.3%    93.0 s       3.0 s \n",
      "INFO  03:24:53,574 DepthOfCoverage - Printing summary info \n",
      "INFO  03:24:53,592 DepthOfCoverage - Printing locus summary \n",
      "INFO  03:24:53,628 ProgressMeter -            done   1139569.0    92.0 s      81.0 s      100.0%    92.0 s       0.0 s \n",
      "INFO  03:24:53,629 ProgressMeter - Total runtime 92.99 secs, 1.55 min, 0.03 hours \n",
      "INFO  03:24:53,636 MicroScheduler - 0 reads were filtered out during the traversal out of approximately 784198 total reads (0.00%) \n",
      "INFO  03:24:53,636 MicroScheduler -   -> 0 reads (0.00% of total) failing BadCigarFilter \n",
      "INFO  03:24:53,637 MicroScheduler -   -> 0 reads (0.00% of total) failing DuplicateReadFilter \n",
      "INFO  03:24:53,637 MicroScheduler -   -> 0 reads (0.00% of total) failing FailsVendorQualityCheckFilter \n",
      "INFO  03:24:53,637 MicroScheduler -   -> 0 reads (0.00% of total) failing MalformedReadFilter \n",
      "INFO  03:24:53,637 MicroScheduler -   -> 0 reads (0.00% of total) failing NotPrimaryAlignmentFilter \n",
      "INFO  03:24:53,638 MicroScheduler -   -> 0 reads (0.00% of total) failing UnmappedReadFilter \n",
      "------------------------------------------------------------------------------------------\n",
      "Done. There were no warn messages.\n",
      "------------------------------------------------------------------------------------------\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "# Run mapping QC\n",
    "COVER_INT=\"-ct 3 -ct 5 -ct 10\" # Intervals of coverage to make the statistics for, can be multiple values; --summaryCoverageThreshold\n",
    "\n",
    "cd $OUTPUT_DIR/alignment/filtered\n",
    "\n",
    "# Get general alignment statistics\n",
    "mkdir $OUTPUT_DIR/qc/qualimap\n",
    "\n",
    "for sample in *.MAPQ${MAPQ_FINAL}.bam\n",
    "do\n",
    "    # PDF version is better for browsing\n",
    "    qualimap bamqc -bam $sample -nt $THREADS -c -outformat PDF -outdir $OUTPUT_DIR/qc/qualimap -outfile ${sample%.*}.qualimap.pdf\n",
    "    mv $OUTPUT_DIR/qc/qualimap/genome_results.txt $OUTPUT_DIR/qc/qualimap/${sample%.*}.genome_results.txt\n",
    "    # HTML version is necessary for multiQC\n",
    "    mkdir -p $OUTPUT_DIR/qc/qualimap/html/${sample%.*}\n",
    "    qualimap bamqc -bam $sample -nt $THREADS -c -outformat HTML -outdir $OUTPUT_DIR/qc/qualimap/html/${sample%.*} # Has to be redirected to separate folder for each file\n",
    "done\n",
    "\n",
    "multiqc -o $OUTPUT_DIR/qc/qualimap $OUTPUT_DIR/qc/qualimap/\n",
    "\n",
    "# Calculate the coverage\n",
    "mkdir -p $OUTPUT_DIR/qc/coverage\n",
    "\n",
    "# Prepare indexes\n",
    "samtools faidx $REFERENCE\n",
    "rm ${REFERENCE%.*}.dict\n",
    "picard CreateSequenceDictionary R=$REFERENCE O=${REFERENCE%.*}.dict\n",
    "\n",
    "# Run GATK DepthOfCoverage\n",
    "ls *.MAPQ40.bam | tr ' ' '\\n' > $OUTPUT_DIR/alignment/filtered//input_bams.list\n",
    "\n",
    "echo \"Going to process files\" `cat $OUTPUT_DIR/alignment/filtered//input_bams.list`\n",
    "\n",
    "gatk \\\n",
    "-T DepthOfCoverage \\\n",
    "-R $REFERENCE \\\n",
    "-I $OUTPUT_DIR/alignment/filtered/input_bams.list \\\n",
    "-o $OUTPUT_DIR/qc/coverage/$(basename $REFERENCE .fa)${INPUT_BAM%.bam}.coverage \\\n",
    "$COVER_INT # --outputFormat csv # Default is readable table (rtable)\n",
    "\n",
    "rm $OUTPUT_DIR/alignment/filtered/input_bams.list\n",
    "\n",
    "pigz -p $THREADS $OUTPUT_DIR/qc/coverage/*.coverage\n",
    "\n",
    "# Count number of mapped reads - we have the same information in Qualimap report\n",
    "echo \"Number of mapped read pairs (-F 3852 -f 2 -q 40 = read mapped in proper pair ; NOT read unmapped, mate unmapped not primary alignment, read fails platform/vendor quality checks, read is PCR or optical duplicate, supplementary alignment)\" > $OUTPUT_DIR/qc/coverage/num_reads.txt\n",
    "\n",
    "for sample in *.MAPQ40.bam\n",
    "do\n",
    "    echo -ne $sample ' '\n",
    "    samtools view -@ $THREADS -F 3852 -f 2 -q 40 $sample | cut -f 1 | sort -T $OUTPUT_DIR/tmp | uniq | wc -l\n",
    "done >> $OUTPUT_DIR/qc/coverage/num_reads.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### Alignment consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this phase, we can generate the mapping consensus. Please not this steps is closely related to the previous step as it will take only the filtered mappings into consideration. Here, we use a 'simple' variant call performed by `samtools`&`bcftools` and `GATK`. You might consider alternating this step with more sophisticated variant call if you think it is necessary. The difference is that the \"samtools\" version gives you lower-case letters where coverage was low and *N* where there was no coverage. Here, we convert all low-coverage positions to *n* (lower-case). The \"GAKT\" version copies the reference sequence where is no coverage. \"samtools\" version doesn't like to include indels whereas \"GATK\" version should be OK with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix reference name and SAM header (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bcftools` and `vcfutils` might have problem with \".\" or any \"strange\" symbol in the reference name. If you haven't \"fix\" your reference name you can do it here but remember you have to replace it in the SAM/BAM header as well. Or go back to the beginning and redo the whole analysis (joke)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the reference name\n",
    "sed -i 's/gi_511533127_gb_CP004011\\.1__Treponema_pallidum_subsp\\._pallidum_SS14,_complete_genome/gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome/g' $REFERENCE \n",
    "\n",
    "# Replace the SAM header\n",
    "for sample in *.filt.indelRealigned.dedup.MAPQ40.bam\n",
    "do\n",
    "    samtools view -@ $THREADS -H $sample > $sample.header.sam  # extract header only\n",
    "\n",
    "    sed -i 's/gi_511533127_gb_CP004011\\.1__Treponema_pallidum_subsp\\._pallidum_SS14,_complete_genome/gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome/g' $sample.header.sam\n",
    "\n",
    "    samtools reheader $sample.header.sam $sample > $sample.tmp\n",
    "    mv $sample.tmp $sample\n",
    "\n",
    "    rm $sample.header.sam\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping consensus ~ alignment-guided assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) [Tue Jun 25 23:15:10 CEST 2019] picard.sam.CreateSequenceDictionary REFERENCE=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa OUTPUT=/mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.dict    TRUNCATE_NAMES_AT_WHITESPACE=true NUM_SEQUENCES=2147483647 VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json\n",
      "[Tue Jun 25 23:15:10 CEST 2019] Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01; Picard version: 2.9.2-SNAPSHOT\n",
      "[Tue Jun 25 23:15:10 CEST 2019] picard.sam.CreateSequenceDictionary done. Elapsed time: 0.01 minutes.\n",
      "Runtime.totalMemory()=514850816\n",
      "(treponema) (.env) (treponema) (.env) [mpileup] 1 samples in 1 input files\n",
      "INFO  23:18:27,845 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  23:18:27,848 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n",
      "INFO  23:18:27,849 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute \n",
      "INFO  23:18:27,849 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk \n",
      "INFO  23:18:27,849 HelpFormatter - [Tue Jun 25 23:18:27 CEST 2019] Executing on Linux 4.4.0-148-generic amd64 \n",
      "INFO  23:18:27,849 HelpFormatter - OpenJDK 64-Bit Server VM 1.8.0_192-b01 \n",
      "INFO  23:18:27,852 HelpFormatter - Program Args: -T FastaAlternateReferenceMaker -R /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa -o /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/consensus/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.cns.gatk.fasta --variant /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/consensus/other/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.vcf \n",
      "INFO  23:18:27,864 HelpFormatter - Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01. \n",
      "INFO  23:18:27,864 HelpFormatter - Date/Time: 2019/06/25 23:18:27 \n",
      "INFO  23:18:27,865 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  23:18:27,865 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  23:18:27,874 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  23:18:27,968 GenomeAnalysisEngine - Downsampling Settings: Method: BY_SAMPLE, Target Coverage: 1000 \n",
      "INFO  23:18:36,244 RMDTrackBuilder - Writing Tribble index to disk for file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/consensus/other/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.vcf.idx \n",
      "INFO  23:18:37,082 GenomeAnalysisEngine - Preparing for traversal \n",
      "INFO  23:18:37,083 GenomeAnalysisEngine - Done preparing for traversal \n",
      "INFO  23:18:37,084 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] \n",
      "INFO  23:18:37,084 ProgressMeter -                 | processed |    time |    per 1M |           |   total | remaining \n",
      "INFO  23:18:37,085 ProgressMeter -        Location |     sites | elapsed |     sites | completed | runtime |   runtime \n",
      "INFO  23:18:45,809 ProgressMeter -            done   1139569.0     8.0 s       7.0 s      100.0%     8.0 s       0.0 s \n",
      "INFO  23:18:45,810 ProgressMeter - Total runtime 8.73 secs, 0.15 min, 0.00 hours \n",
      "------------------------------------------------------------------------------------------\n",
      "Done. There were no warn messages.\n",
      "------------------------------------------------------------------------------------------\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "MIN_COVER=3 # All bases bellow this coverage are converted to ns\n",
    "PLOIDY=1 # Ploidy for the bcftools\n",
    "\n",
    "mkdir -p $OUTPUT_DIR/consensus/other\n",
    "\n",
    "# Make sure we have reference index and dictionary (! have to redo if you used the previous \"fix\" step)\n",
    "samtools faidx $REFERENCE\n",
    "rm ${REFERENCE%.*}.dict\n",
    "picard CreateSequenceDictionary R=$REFERENCE O=${REFERENCE%.*}.dict\n",
    "\n",
    "for sample in *.filt.indelRealigned.dedup.MAPQ40.bam\n",
    "do\n",
    "    samtools index -@ $THREADS $sample\n",
    "    samtools mpileup --max-depth 10000 -E --min-BQ 13 --fasta-ref $REFERENCE --min-MQ 40 -g -u --max-idepth 100000 --min-ireads 5 --gap-frac 0.002 --excl-flags UNMAP,SECONDARY,QCFAIL,DUP --output-tags DP,AD,ADF,ADR,SP,INFO/AD,INFO/ADF,INFO/ADR --reference $REFERENCE $sample | bcftools call --ploidy $PLOIDY -c --keep-masked-ref --output-type v --threads $THREADS - > $OUTPUT_DIR/consensus/other/${sample%.*}.vcf\n",
    "\n",
    "    cat $OUTPUT_DIR/consensus/other/${sample%.*}.vcf | vcfutils.pl vcf2fq -d $MIN_COVER | seqtk seq -A - > $OUTPUT_DIR/consensus/${sample%.*}.cns.def.fasta\n",
    "\n",
    "    # Convert all low-coverage (=lower-case) positions to \"n\"\n",
    "    sed -e '/^>/! s/[[:lower:]]/n/g' $OUTPUT_DIR/consensus/${sample%.*}.cns.def.fasta > $OUTPUT_DIR/consensus/${sample%.*}.cns.final.fasta\n",
    "\n",
    "    gatk -T FastaAlternateReferenceMaker -R $REFERENCE -o $OUTPUT_DIR/consensus/${sample%.*}.cns.gatk.fasta --variant $OUTPUT_DIR/consensus/other/${sample%.*}.vcf\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### BAM downsampling (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In some cases, our resulting SAM/BAM is very deep which causes problems during the subsequent analysis. An option to lower down the computational demands is to downsample to a specific depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) INFO\t2019-06-26 00:05:24\tSortingCollection\tCreating merging iterator from 16 files\n",
      "[INFO][Biostar154220]Alloc memory for contig gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome N=1139569*sizeof(int)\n",
      "[INFO][Biostar154220]Last M70401:55:000000000-B24PV:1:2104:14484:21063;gi_511533127_gb_CP004011_1_Treponema_pallidum_subsp_pallidum_SS14_complete_genome;339,874 11 seconds\n",
      "[INFO][Biostar154220]. Completed. N=1,179,007. That took:18 seconds\n",
      "[INFO][SortSamRefName]done: N=786004\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "MAX_COVER=250 # Set maximum coverage to put the limit on\n",
    "\n",
    "for sample in *.filt.indelRealigned.dedup.MAPQ40.bam\n",
    "do\n",
    "    java -jar -Xmx4g $CONDA_PREFIX/bin/sortsamrefname.jar --tmpDir $OUTPUT_DIR/tmp $sample |  java -jar -Xmx4g $CONDA_PREFIX/bin/downsamplebam.jar -n $MAX_COVER | samtools sort -@ $THREADS - > ${sample%.bam*}.downsamp${MAX_COVER}.bam\n",
    "    samtools index -@ $THREADS ${sample%.bam*}.downsamp${MAX_COVER}.bam\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Variant call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To get the most relevant variants directly from the alignments we could either use the variants used for the consensus generation or we can use probably more suitable tools for bacteria variant call such as [`freebayes`](https://github.com/ekg/freebayes). The reason why we don't use other tools than `samtools`/`bcftools` for the generation of consensus is the compatibility of the approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Processing CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.bam\n",
      "(treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) (treponema) (.env) Processing  CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.full.vcf\n",
      "INFO  01:18:51,060 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  01:18:51,063 HelpFormatter - The Genome Analysis Toolkit (GATK) v3.7-0-gcfedb67, Compiled 2016/12/12 11:21:18 \n",
      "INFO  01:18:51,063 HelpFormatter - Copyright (c) 2010-2016 The Broad Institute \n",
      "INFO  01:18:51,063 HelpFormatter - For support and documentation go to https://software.broadinstitute.org/gatk \n",
      "INFO  01:18:51,063 HelpFormatter - [Wed Jun 26 01:18:51 CEST 2019] Executing on Linux 4.4.0-148-generic amd64 \n",
      "INFO  01:18:51,064 HelpFormatter - OpenJDK 64-Bit Server VM 1.8.0_192-b01 \n",
      "INFO  01:18:51,067 HelpFormatter - Program Args: -T VariantFiltration -R /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/reference/SS14.fa -o /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/variants/freebayes/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.filt.vcf --variant:VCF CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.full.vcf --filterExpression DP < 3 --filterName LowCoverage --filterExpression QUAL > 0 && QUAL < 20 --filterName VeryLowQual --filterExpression QUAL > 20 && QUAL < 50 --filterName LowQual --filterExpression QUAL == 0 --filterName TechnicalQual \n",
      "INFO  01:18:51,079 HelpFormatter - Executing as 325073@BioDA-server on Linux 4.4.0-148-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_192-b01. \n",
      "INFO  01:18:51,079 HelpFormatter - Date/Time: 2019/06/26 01:18:51 \n",
      "INFO  01:18:51,079 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  01:18:51,080 HelpFormatter - -------------------------------------------------------------------------------- \n",
      "INFO  01:18:51,097 GenomeAnalysisEngine - Strictness is SILENT \n",
      "INFO  01:18:51,186 GenomeAnalysisEngine - Downsampling Settings: Method: BY_SAMPLE, Target Coverage: 1000 \n",
      "INFO  01:18:51,588 RMDTrackBuilder - Writing Tribble index to disk for file /mnt/ssd/ssd_2/bioda_temp/honza/scratch_default/linda/results/variants/freebayes/CW56_S1_.SS14.filt.indelRealigned.dedup.MAPQ40.full.vcf.idx \n",
      "INFO  01:18:51,760 GenomeAnalysisEngine - Preparing for traversal \n",
      "INFO  01:18:51,761 GenomeAnalysisEngine - Done preparing for traversal \n",
      "INFO  01:18:51,761 ProgressMeter - [INITIALIZATION COMPLETE; STARTING PROCESSING] \n",
      "INFO  01:18:51,762 ProgressMeter -                 | processed |    time |    per 1M |           |   total | remaining \n",
      "INFO  01:18:51,762 ProgressMeter -        Location |     sites | elapsed |     sites | completed | runtime |   runtime \n",
      "INFO  01:18:52,779 ProgressMeter -            done      7677.0     1.0 s       2.2 m       99.9%     1.0 s       0.0 s \n",
      "INFO  01:18:52,780 ProgressMeter - Total runtime 1.02 secs, 0.02 min, 0.00 hours \n",
      "------------------------------------------------------------------------------------------\n",
      "Done. There were no warn messages.\n",
      "------------------------------------------------------------------------------------------\n",
      "(treponema) (.env) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "MIN_DP=$MIN_COVER # Set minimal coverage depth of the mapping\n",
    "QUAL_SET=50 # Set minimal variant quality for the most certain variants; default it 20\n",
    "\n",
    "mkdir -p $OUTPUT_DIR/variants/freebayes/stats\n",
    "\n",
    "for sample in *filt.indelRealigned.dedup.MAPQ40.bam\n",
    "do\n",
    "    echo \"Processing $sample\"\n",
    "    \n",
    "    samtools index $sample\n",
    "\n",
    "    freebayes --bam $sample --vcf $OUTPUT_DIR/variants/freebayes/${sample%.*}.full.vcf --fasta-reference $REFERENCE --theta 0.001 --ploidy $PLOIDY --min-mapping-quality 10 --min-base-quality 15 --min-alternate-fraction 0.01 --min-alternate-count 2 --genotype-qualities --report-all-haplotype-alleles # You can add \"--report-monomorphic\" to get all positions\n",
    "done\n",
    "\n",
    "# Variant post-processing and filtering\n",
    "cd $OUTPUT_DIR/variants/freebayes/\n",
    "\n",
    "for sample in *full.vcf\n",
    "do\n",
    "    echo \"Processing \" $sample\n",
    "\n",
    "    # GATK filtering\n",
    "    gatk -T VariantFiltration -R $REFERENCE -o $OUTPUT_DIR/variants/freebayes/${sample%.full*}.filt.vcf --variant:VCF $sample --filterExpression \"DP < ${MIN_DP}\" --filterName \"LowCoverage\" --filterExpression \"QUAL > 0 && QUAL < 20\" --filterName \"VeryLowQual\" --filterExpression \"QUAL > 20 && QUAL < ${QUAL_SET}\" --filterName \"LowQual\" --filterExpression \"QUAL == 0\" --filterName \"TechnicalQual\" # This might be too strict for FreeBayes\n",
    "    # gatk -T VariantFiltration -R $REFERENCE -o ${sample%.full*}.filt.vcf --variant:VCF $sample --filterExpression \"DP < ${MIN_DP}\" --filterName \"LowCoverage\" --filterExpression \"QUAL > 0 && QUAL < 20\" --filterName \"VeryLowQual\" --filterExpression \"QUAL == 0\" --filterName \"TechnicalQual\" # This would be the way FreeBayes author recommends to use the filtering\n",
    "\n",
    "    mv ${sample%.full*}.filt.vcf $sample\n",
    "    rm ${sample%.full*}.filt.vcf.idx\n",
    "    \n",
    "    # Get only variants (PASS filtering)\n",
    "    grep \"^#\" $sample > ${sample%.full*}.var.vcf # Get only header\n",
    "    awk '$7 == \"PASS\" {print $0}' $sample >> ${sample%.full*}.var.vcf\n",
    "\n",
    "    # Get only SNPs and/or indels\n",
    "#    vcffilter -f \"TYPE = snp\" ${sample%.full*}.var.vc > ${sample%.full*}.var.snp.vcf # Get only SNP\n",
    "#    vcffilter -f \"( TYPE = ins | TYPE = del )\" ${sample%.full*}.var.vcf > ${sample%.full*}.var.indel.vcf # Get only indels\n",
    "\n",
    "    # Convert vct to tab vcftools\n",
    "    cat ${sample%.full*}.var.vcf | vcf-to-tab > ${sample%.full*}.var.tsv\n",
    "\n",
    "    # Get stats of the variant calling\n",
    "    # Stats from raw and filtered SNPs\n",
    "    vcfstats $sample > $OUTPUT_DIR/variants/freebayes/stats/${sample%.vcf}.stats.txt\n",
    "    vcfstats ${sample%.full*}.var.vcf > $OUTPUT_DIR/variants/freebayes/stats/${sample%.full*}.var.stats.txt\n",
    "\n",
    "    # Compare ts/tv ration between high/low quality variants\n",
    "    echo $sample > $OUTPUT_DIR/variants/freebayes/stats/${sample%.vcf}.ratio.stats.txt\n",
    "    echo \"Low quality variants\" >> $OUTPUT_DIR/variants/freebayes/stats/${sample%.*}.ratio.stats.txt # Low quality variants\n",
    "    vcffilter -f \"QUAL < 20\" $sample | vcfstats | grep \"ts\\|bial\" >> $OUTPUT_DIR/variants/freebayes/stats/${sample%.vcf}.ratio.stats.txt\n",
    "    echo \"High quality variants\" >> $OUTPUT_DIR/variants/freebayes/stats/${sample%.vcf}.ratio.stats.txt # High quality variants\n",
    "    vcffilter -f \"QUAL > 20\" $sample | vcfstats | grep \"ts\\|bial\" >> $OUTPUT_DIR/variants/freebayes/stats/${sample%.vcf}.ratio.stats.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### *De novo* assembly - mapped reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To accompany the alignment-guided assembly and to balance for the potentical pitfalls of that approach we strongly recommend to run *de novo* assembly as well. You can run *de novo* assembly on the host-cleaned data but since we assume there could be a mixture of organisms present we recommend to run the assembly only from the initial mapping step (\"rough mapping\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAM/BAM to fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preparation step, we first extract the mapped reads from the SAM/BAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cd $OUTPUT_DIR/alignment\n",
    "\n",
    "mkdir -p $OUTPUT_DIR/alignment/fastq\n",
    "\n",
    "for sample in *.$(basename $REFERENCE .fa).bam\n",
    "do\n",
    "    samtools view -H $sample > ${sample%.*}.sam\n",
    "\n",
    "    samtools view -@ $THREADS -F 4 -f 3 $sample  >> ${sample%.*}.sam\n",
    "    samtools view -@ $THREADS -b ${sample%.*}.sam | samtools sort -@ $THREADS -o ${sample%.*}.mappedOnly.bam -\n",
    "    rm ${sample%.*}.sam\n",
    "\n",
    "    picard FixMateInformation I=${sample%.*}.mappedOnly.bam O=tmp.bam VALIDATION_STRINGENCY=LENIENT\n",
    "    mv tmp.bam ${sample%.*}.mappedOnly.bam\n",
    "    samtools index -@ $THREADS ${sample%.*}.mappedOnly.bam\n",
    "\n",
    "    samtools collate -O -@ $THREADS ${sample%.*}.mappedOnly.bam - | samtools fastq -@ $THREADS -s $SCRATCH/fastq/${sample%.*}_unpaired.mappedOnly.fastq -1 $SCRATCH/fastq/${sample%.*}_R1.mappedOnly.fastq -2 $SCRATCH/fastq/${sample%.*}_R2.mappedOnly.fastq -\n",
    "done\n",
    "\n",
    "cd $OUTPUT_DIR/alignment/fastq/\n",
    "\n",
    "rm *_unpaired.mappedOnly.fastq\n",
    "\n",
    "for sample in *.fastq\n",
    "do\n",
    "    pigz -p $THREADS $sample\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *De novo* assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the *de novo* assembly itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l select=1:ncpus=10:mem=10gb:scratch_local=200gb\n",
    "#PBS -l walltime=24:00:00\n",
    "#PBS -N 08b_assembly\n",
    "#\n",
    "# Assemble cleaned and reference mapped reads\n",
    "# This assembly is supposed to run after bam2fastq.sh\n",
    "#\n",
    "# Requires SPAdes, Quast\n",
    "#\n",
    "\n",
    "####################################################################################################\n",
    "### Variables\n",
    "INPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/linda_treponema/2018_jan_2/results/SS14/alignment/mappedOnly/fastq\n",
    "OUTPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/linda_treponema/2018_jan_2/results/assembly/mappedOnly\n",
    "\n",
    "KMERS=\"15,21,27,33,55,77,99,127\" # List of kmers for the assembly\n",
    "\n",
    "# If we wan't to add this to Quast evaluation\n",
    "REF_SEQ=SS14.fa # SS14.fa; nichols.fa; bosnia_a.fa\n",
    "REF_DIR=/storage/brno2/home/opplatek/genomes/treponema\n",
    "\n",
    "NSEQ=10 # How many top sequences we will report in short result\n",
    "\n",
    "APPENDIX=\".mappedOnly.fastq.gz\"\n",
    "APPENDIX1=\"_1.trimmed.clean.mappedOnly.fastq.gz\"\n",
    "APPENDIX2=\"_2.trimmed.clean.mappedOnly.fastq.gz\"\n",
    "\n",
    "THREADS=$PBS_NUM_PPN\n",
    "\n",
    "RAM=$[(($PBS_RESC_MEM/1024)/1024)/1024] # Convert bits to gigabits ))\n",
    "RAM=$[$RAM-1]\n",
    "MEM_LIMIT=$RAM # $[$RAM/$THREADS] # this is in GB and not in Gb like they say in the manual - IMPORTANT SPAdes takes this memory limit for EACH thread and not as memory limit in total for whole calculation\n",
    "#MEM_LIMIT=$(printf \"%.0f\" $MEM_LIMIT) # And round up\n",
    "\n",
    "SPADES=/storage/brno2/home/opplatek/tools/SPAdes-3.10.1-Linux/bin/spades.py\n",
    "QUAST=/storage/brno2/home/opplatek/tools/quast-4.5/quast.py\n",
    "\n",
    "####################################################################################################\n",
    "### Copy inputs\n",
    "cp $INPUT_DIR/*$APPENDIX $SCRATCH/\n",
    "\n",
    "cp $REF_DIR/$REF_SEQ $SCRATCH/\n",
    "\n",
    "cd $SCRATCH/\n",
    "\n",
    "####################################################################################################\n",
    "### Assembly reads\n",
    "mkdir -p $SCRATCH/tmp\n",
    "mkdir -p $SCRATCH/spades\n",
    "\n",
    "for sample in *$APPENDIX1\n",
    "#for sample in ${SAMPLES}*R1*\n",
    "do\n",
    "\tFORWARD=$sample\n",
    "\tREVERSE=${FORWARD%${APPENDIX1}*}${APPENDIX2}\n",
    "#\textension=\"${FORWARD##*R1}\"\n",
    "#\tREVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "#\tAPPENDIX1=_R1${extension}\n",
    "#\tAPPENDIX2=_R2${extension}\n",
    "\tNAME=${FORWARD%${APPENDIX1}*}\n",
    "\n",
    "\tmkdir $SCRATCH/spades/$NAME\n",
    "\n",
    "\t$SPADES --careful --threads $THREADS --memory $MEM_LIMIT --cov-cutoff 5 --tmp-dir $SCRATCH/tmp -k $KMERS -1 $FORWARD -2 $REVERSE -o $SCRATCH/spades/$NAME\n",
    "done\n",
    "\n",
    "####################################################################################################\n",
    "### Run assembly statistics\n",
    "cd $SCRATCH/spades/\n",
    "\n",
    "# Get only sequences longer than 10000 bp\n",
    "#for i in */*.scaffolds.fasta\n",
    "#do\n",
    "#\tpython ~/Documents/scripts_commands/filterMultiFastaByLength.py $i 10000 1000000 ${i%.*}.filtered.fasta\n",
    "#done\n",
    "\n",
    "mkdir $SCRATCH/spades/scaffolds\n",
    "mkdir $SCRATCH/spades/graphs\n",
    "\n",
    "# Copy all scaffolds to one folder\n",
    "for i in $(ls -d */)\n",
    "do\n",
    "\tcat $i/scaffolds.fasta > $SCRATCH/spades/scaffolds/${i%/*}.scaffolds.fasta\n",
    "\tcat $i/assembly_graph.fastg > $SCRATCH/spades/graphs/${i%/*}.assembly_graph.fastg\n",
    "\tcat $i/assembly_graph_with_scaffolds.gfa > $SCRATCH/spades/graphs/${i%/*}.assembly_graph_with_scaffolds.gfa\n",
    "done\n",
    "\n",
    "cd $SCRATCH/spades/scaffolds\n",
    "\n",
    "module add python27-modules-gcc # For python-matplotlib\n",
    "$QUAST --threads $THREADS $(ls *.scaffolds.fasta) # -s -> scaffolds\n",
    "\n",
    "# Get only first X sequences\n",
    "for i in *.fasta\n",
    "do\n",
    "\tawk \"/^>/ {n++} n>$NSEQ {exit} {print}\" $i > ${i%.*}.top${NSEQ}.fasta\n",
    "done\n",
    "\n",
    "# You can view the output FASTG (for SPAdes) in Bandage https://github.com/rrwick/Bandage/wiki/Getting-started\n",
    "#export QT_SELECT=5 # Make sure you are using correct Qt version\n",
    "#Bandage load assembly_graph.fastg\n",
    "\n",
    "############################################################################################\n",
    "### Copy results\n",
    "mkdir -p $OUTPUT_DIR/spades/full\n",
    "\n",
    "cp -r $SCRATCH/spades/scaffolds $OUTPUT_DIR/spades/\n",
    "cp -r $SCRATCH/spades/graphs $OUTPUT_DIR/spades/\n",
    "cp -r $SCRATCH/spades $OUTPUT_DIR/spades/full/\n",
    "\n",
    "rm -r $SCRATCH/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Additional scaffolding (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l select=1:ncpus=6:mem=10gb:scratch_local=200gb\n",
    "#PBS -l walltime=04:00:00\n",
    "#PBS -N 09_scaffolding\n",
    "#\n",
    "# Additional scaffolding for genome assembly\n",
    "#\n",
    "# Requires BESST, BWA, Samtools, Quast\n",
    "#\n",
    "####################################################################################################\n",
    "#\n",
    "# Extra additional scaffolding using BESST https://github.com/ksahlin/BESST \n",
    "#\tTODO\tand SSPACE https://www.baseclear.com/genomics/bioinformatics/basetools/SSPACE; https://github.com/nsoranzo/sspace_basic\n",
    "#\n",
    "# For proper function we need to get instert size - either we have it from the sequencing of we have to get it from mapping of raw sequencing reads to the draft assembly\n",
    "#\n",
    "\n",
    "####################################################################################################\n",
    "### Variables\n",
    "INPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/linda_treponema/2017_oct/results/ss14/alignment/mappedOnly/fastq # Input reads\n",
    "INPUT_SCAFFOLDS_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/linda_treponema/2017_oct/results/assembly/mappedOnly/spades/scaffolds # Input scaffolds\n",
    "OUTPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/linda_treponema/2017_oct/results/assembly/mappedOnly/spades/besst\n",
    "\n",
    "# If we wan't to add this to Quast evaluation\n",
    "REF_SEQ=SS14.fa # SS14.fa; nichols.fa; bosnia_a.fa\n",
    "REF_DIR=/storage/brno2/home/opplatek/genomes/treponema\n",
    "\n",
    "THREADS=$PBS_NUM_PPN\n",
    "\n",
    "module add bwa-0.7.15\n",
    "BWA=bwa\n",
    "module add samtools-1.4\n",
    "SAMTOOLS=samtools\n",
    "BESST_BIN=/storage/brno2/home/opplatek/tools/BESST-2.2.4/bin\n",
    "QUAST=/storage/brno2/home/opplatek/tools/quast-4.5/quast.py\n",
    "#SSPACE=/storage/brno2/home/opplatek/tools/SSPACE/SSPACE-STANDARD-3.0_linux-x86_64/SSPACE_Standard_v3.0.pl # TODO\n",
    "#GAP_FILLER=/storage/brno2/home/opplatek/tools/SSPACE/GapFiller_v1-10_linux-x86_64/GapFiller.pl # Distribution along SSPACE; TODO\n",
    "\n",
    "####################################################################################################\n",
    "### Copy inputs\n",
    "cp $INPUT_SCAFFOLDS_DIR/*.scaffolds.fasta $SCRATCH/\n",
    "cp $REF_DIR/$REF_SEQ $SCRATCH/\n",
    "cp $INPUT_DIR/*.fastq.gz $SCRATCH/\n",
    "\n",
    "cd $SCRATCH/\n",
    "\n",
    "####################################################################################################\n",
    "### Creat virtual environemnt\n",
    "# There are some troubles with dependencies - when loading python27-modules-gcc there is an error with htslib; need to install own virtualenv, pip and numpy http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/\n",
    "\n",
    "# Activate virtualenv and install\n",
    "module add python27-modules-gcc\n",
    "#virtualenv $SCRATCH/my_project # DO NOT USE - CREATES ERROR\n",
    "virtualenv -p /usr/bin/python2.7 $SCRATCH/python_env_BESST\n",
    "export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python2.7\n",
    "source $SCRATCH/python_env_BESST/bin/activate # Activate\n",
    "module rm python27-modules-gcc\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "export PYTHONPATH=$SCRATCH/python_env_BESST/lib/python2.7/site-packages:$PYTHONPATH\n",
    "export PYTHONPATH=/storage/brno2/home/opplatek/tools/BESST-2.2.4/lib/python2.7/site-packages:$PYTHONPATH\n",
    "\n",
    "####################################################################################################\n",
    "### BESST scaffolding\n",
    "mkdir -p $SCRATCH/BESST\n",
    "\n",
    "for sample in *R1*.fastq.gz\n",
    "do\n",
    "\tFORWARD=$sample\n",
    "\textension=${FORWARD##*R1}\n",
    "\tREVERSE=${FORWARD%R1*}R2${extension}\n",
    "\n",
    "\tCONTIG=${FORWARD%_R1*}.scaffolds.fasta # SPAdes assembly contigs/scaffolds\n",
    "\n",
    "\t$BWA index $CONTIG\n",
    "\n",
    "\t$BWA mem -t $THREADS -w 0 -O 99 $CONTIG $FORWARD $REVERSE | $SAMTOOLS view -F 4 -@ $THREADS -b - | $SAMTOOLS fixmate -O bam - - | $SAMTOOLS sort -@ $THREADS - > $SCRATCH/BESST/${FORWARD%_R1*}${extension%.fastq.gz*}.bam # Make alignment for BESST; default is BWA-MEM and recommended settings are used; they have script to do it (reads_to_ctg_map.py) but it has some error - setting taken from there\n",
    "\n",
    "\t$SAMTOOLS index -@ $THREADS $SCRATCH/BESST/${FORWARD%_R1*}${extension%.fastq.gz*}.bam\n",
    "\n",
    "\tmkdir $SCRATCH/BESST/${FORWARD%%.*}\n",
    "\n",
    "\t$BESST_BIN/runBESST -c $CONTIG -f $SCRATCH/BESST/${FORWARD%_R1*}${extension%.fastq.gz*}.bam -filter_contigs 100 --orientation fr -o $SCRATCH/BESST/${FORWARD%%.*}\n",
    "done\n",
    "\n",
    "deactivate  # Deactivate virtual environment\n",
    "rm -rf $SCRATCH/python_env_BESST # Remove virtual environment\n",
    "\n",
    "####################################################################################################\n",
    "### Write and sort final scaffolds\n",
    "cd $SCRATCH/BESST\n",
    "\n",
    "mkdir -p $SCRATCH/BESST_scaffolds\n",
    "\n",
    "for i in $(ls -d */)\n",
    "do\n",
    "\tcat $i/BESST_output/pass1/Scaffolds_pass1.fa > $SCRATCH/BESST_scaffolds/${i%/*}.scaffolds.fasta\n",
    "done\n",
    "\n",
    "# Sort by length https://www.biostars.org/p/153999/\n",
    "cd $SCRATCH/BESST_scaffolds/\n",
    "\n",
    "for i in *.fasta\n",
    "do\n",
    "\tcat $i | awk '/^>/ {printf(\"%s%s\\t\",(N>0?\"\\n\":\"\"),$0);N++;next;} {printf(\"%s\",$0);} END {printf(\"\\n\");}' | awk -F '\\t' '{printf(\"%d\\t%s\\n\",length($2),$0);}' | sort -k1,1nr | cut -f 2- | tr \"\\t\" \"\\n\" > $i.tmp # Longest first; shortest first change sort 'sort -k1,1nr' to 'sort -k1,1n'\n",
    "\tmv $i.tmp $i\n",
    "done\n",
    "\n",
    "$QUAST --threads $THREADS -R $SCRATCH/$REF_SEQ $(ls *.scaffolds.fasta) # -s -> scaffolds\n",
    "\n",
    "############################################################################################\n",
    "### Copy results\n",
    "mkdir -p $OUTPUT_DIR\n",
    "\n",
    "cp -r $SCRATCH/BESST_scaffolds/* $OUTPUT_DIR/\n",
    "\n",
    "rm -rf $SCRATCH/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Assembly quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=1:0:0 \n",
    "#PBS -l select=1:ncpus=6\n",
    "#PBS -l mem=2gb\n",
    "#PBS -l scratch_local=50gb\n",
    "#PBS -N busco\n",
    "#\n",
    "# BUSCO assembly check\n",
    "# Originaly designed for E.coli\n",
    "# \tFor other see BUSCO databases and change accordingly\n",
    "# For E. coli it takes ~5 minutes so it's not necessary to run it on a cluster\n",
    "\n",
    "INPUT_FILE=/storage/brno2/home/opplatek/bioinf_projects/biocore/katarina_assembly/results/assembly/unicycler/assembly.fasta\n",
    "OUTPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/katarina_assembly/results/qc/assembly/BUSCO/$(basename INPUT_FILE .fasta)\n",
    "\n",
    "LINEAGE=/storage/brno2/home/opplatek/genomes/busco/enterobacteriales_odb9.tar.gz # BUSCO lineage\n",
    "\n",
    "BUSCO_INI=/storage/brno2/home/opplatek/tools/anaconda3/envs/busco/share/busco-3.0.2-6/conda.config.ini\n",
    "THREADS=$PBS_NUM_PPN\n",
    "\n",
    "# Install BUSCO in Anaconda\n",
    "#/storage/brno2/home/opplatek/tools/anaconda3/bin/conda create -n busco python=3.6\n",
    "#source /storage/brno2/home/opplatek/tools/anaconda3/bin/activate busco\n",
    "#/storage/brno2/home/opplatek/tools/anaconda3/bin/conda install -c bioconda busco \n",
    "\n",
    "source /storage/brno2/home/opplatek/tools/anaconda3/bin/activate busco\n",
    "module add blast+-2.2.29 # There are some problems with multiple cores runs with newer BLAST and \"downgrading\" to 2.2.xxx helps it. If we still get an error we have to go for a single core onlyt\n",
    "\n",
    "################################################################################\n",
    "# \n",
    "cp $INPUT_FILE $SCRATCH/\n",
    "cp $BUSCO_INI $SCRATCH/busco.ini\n",
    "\n",
    "cp $LINEAGE $SCRATCH/\n",
    "\n",
    "export BUSCO_CONFIG_FILE=$SCRATCH/busco.ini\n",
    "\n",
    "tar xvzf $SCRATCH/$(basename $LINEAGE)\n",
    "\n",
    "# Augustus needs to write something to the installation folder - on MetaCentrum we try to avoid extensive writing to the storage\n",
    "cp -r /storage/brno2/home/opplatek/tools/anaconda3/pkgs/augustus-3.2.3-boost1.60_0/config $SCRATCH/augustus_config\n",
    "export AUGUSTUS_CONFIG_PATH=$SCRATCH/augustus_config\n",
    "\n",
    "INPUT_FILE=$(basename $INPUT_FILE)\n",
    "\n",
    "################################################################################\n",
    "cd $SCRATCH/\n",
    "\n",
    "#mkdir $SCRATCH/busco_${INPUT_FILE%.fa*}\n",
    "run_BUSCO.py -i $INPUT_FILE -o busco_${INPUT_FILE%.fa*} -l $SCRATCH/$(basename $LINEAGE .tar.gz) -m geno \\\n",
    "-c $THREADS -sp E_coli_K12 -t $SCRATCH\n",
    "\n",
    "################################################################################\n",
    "mkdir $OUTPUT_DIR\n",
    "cp -r $SCRATCH/run_busco_${INPUT_FILE%.fa*} $OUTPUT_DIR/ || exit 1\n",
    "\n",
    "rm -r $SCRATCH/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Scaffold contamination scan (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#PBS -l walltime=01:00:00\n",
    "#PBS -l select=1:ncpus=6:mem=160gb:scratch_local=200gb\n",
    "#PBS -N kraken_scan_scaffolds_P101HW18100699\n",
    "#PBS -q default\n",
    "#\n",
    "# Kraken scan fastq.gz\n",
    "# Designed for assembled scaffolds in uncompressed fasta\n",
    "# For paired-end or single-end fastq.gz please see Kraken2 manual https://ccb.jhu.edu/software/kraken2/index.shtml?t=manual#masking-of-low-complexity-sequences\n",
    "#\n",
    "#Note: Kraken2 read annotation - it would be too simple if Kraken2 would put the �real� taxid into the annotated fasta/fastq output\n",
    "#\tInstead, it puts there complete shit\n",
    "#\tBut, we can take a look into xxx.kraken.txt, get second and third column and from there extract the sequence by name we want from either the original fasta/fastq file or from xxx.classified-out.fasta\n",
    "#\n",
    "\n",
    "INPUT_DIR=/storage/brno2/home/opplatek/bioinf_projects/biocore/smajs_treponema/11_2018/results/assembly/P101HW18100699/all_reads/spades/scaffolds\n",
    "OUTPUT_DIR=${INPUT_DIR%/assembly*}/qc/kraken/P101HW18100699/spades_scaffolds_nonames\n",
    "\n",
    "DBNAME=/storage/plzen1/home/opplatek/genome/kraken_db/122018/kraken_db_122018\n",
    "\n",
    "SUFFIX=\".scaffolds.fasta\"\n",
    "SUFFIX1=$SUFFIX\n",
    "#SUFFIX2=\"_2.fq.gz\"\n",
    "\n",
    "THREADS=$PBS_NUM_PPN\n",
    "\n",
    "KRAKEN_BIN=/storage/brno2/home/opplatek/tools/kraken2-2.0.7-beta\n",
    "\n",
    "cp -r $DBNAME $SCRATCH/ &\n",
    "DBNAME=$(basename $DBNAME)\n",
    "\n",
    "cp $INPUT_DIR/*$SUFFIX $SCRATCH/\n",
    "\n",
    "wait\n",
    "\n",
    "cd $SCRATCH/\n",
    "\n",
    "mkdir $SCRATCH/kraken\n",
    "\n",
    "for i in *$SUFFIX1\n",
    "do\n",
    "\tFORWARD=$i\n",
    "\tREVERSE=${i%$SUFFIX1*}$SUFFIX2\n",
    "\t$KRAKEN_BIN/kraken2 --threads $THREADS --use-names --classified-out $SCRATCH/kraken/${i%$SUFFIX1*}.classified-out.fasta --unclassified-out $SCRATCH/kraken/${i%$SUFFIX1*}.unclassified-out.fasta --output $SCRATCH/kraken/${i%$SUFFIX1*}.kraken.txt --report $SCRATCH/kraken/${i%$SUFFIX1*}.kraken.report.txt --db $SCRATCH/$DBNAME $SCRATCH/$FORWARD # --gzip-compressed\n",
    "\n",
    "#\tfor a in *.unclassified*.fasta\n",
    "#\tdo\n",
    "#\t\tpigz -p $THREADS $a\n",
    "#\tdone \n",
    "\n",
    "#\tfor a in *.classified*.fasta\n",
    "#\tdo\n",
    "#\t\tpigz -p $THREADS $a\n",
    "#\tdone \n",
    "\n",
    "# Extract only the annotated reads and their taxa\n",
    "cat $SCRATCH/kraken/${i%$SUFFIX1*}.kraken.txt | cut -f2,3 $i > $SCRATCH/kraken/${i%$SUFFIX1*}.kraken.taxid.txt\n",
    "\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "mkdir -p $OUTPUT_DIR\n",
    "\n",
    "cp -r $SCRATCH/kraken/* $OUTPUT_DIR/\n",
    "\n",
    "rm -r $SCRATCH/*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gene prediction (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#\n",
    "# Gene prediction and annotation using prokka https://github.com/tseemann/prokka\n",
    "#\n",
    "# There could be another annotation by RAST (http://rast.nmpdr.org/) Augustus (http://bioinf.uni-greifswald.de/webaugustus/index.gsp) or funannotate (http://funannotate.readthedocs.io/en/latest/index.html) or rast (http://rast.nmpdr.org/)\n",
    "# Probably the \"nicest\" looking are from Prokka and RAST\n",
    "\n",
    "\n",
    "INPUT_ASSEMBLY=/home/jan/Data/projects/katarina_assembly/results/assembly/unicycler/Unicycler.fasta\n",
    "OUTPUT_DIR=/home/jan/Data/projects/katarina_assembly/results/annotation/unicycler/prokka\n",
    "GBK=/home/jan/Data/projects/katarina_assembly/data/GCF_000005845.2_ASM584v2_genomic.gbk # This has to be highly trusted set of genes which will be annotated first; downloaded in GenBank from https://www.ncbi.nlm.nih.gov/genome/?term=Escherichia+coli+str.+K-12+substr.+MG1655\n",
    "\n",
    "THREADS=6\n",
    "\n",
    "source activate prokka\n",
    "PROKKA=$(which prokka)\n",
    "\n",
    "$PROKKA --cpus $THREADS --proteins $GBK --kingdom Bacteria --outdir $OUTPUT_DIR --prefix $(basename $INPUT_ASSEMBLY .fasta) $INPUT_ASSEMBLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Whole genome alignment (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#\n",
    "# Align contigs to reference genomes and get come figures\n",
    "#\n",
    "\n",
    "MUMMER_BIN=/home/jan/Tools/mummer-3.9.4alpha\n",
    "\n",
    "REF_DIR=\"/home/jan/Desktop/04162019_iva/references\"\n",
    "QRY_DIR=\"/home/jan/Desktop/04162019_iva/scaffolds\"\n",
    "\n",
    "for REF_FASTA in $REF_DIR/*_genomic.fa\n",
    "do\n",
    "\techo \"Running $REF_FASTA.\"\n",
    "\n",
    "\tfor QRY_FASTA in $QRY_DIR/*.scaffolds.fasta\n",
    "\tdo\n",
    "\t\techo \"Running $QRY_FASTA.\"\n",
    "\n",
    "\t#\tREF_FASTA=/home/jan/Desktop/04162019_iva/references/Sep_GCF_000007645.1_ASM764v1_genomic.fa\n",
    "\t#\tQRY_FASTA=/home/jan/Desktop/04162019_iva/scaffolds/CoNS2.scaffolds.fasta\n",
    "\n",
    "\t\tQRY_NAME=$(basename $QRY_FASTA .fasta)\n",
    "\t\tREF_NAME=$(basename $REF_FASTA .fa)\n",
    "\n",
    "\t\techo \"Aligning $QRY_NAME to $REF_NAME.\"\n",
    "\n",
    "\t\tmkdir -p alignments/$REF_NAME/$QRY_NAME\n",
    "\n",
    "\t\t$MUMMER_BIN/nucmer -p alignments/$REF_NAME/$QRY_NAME/nucmer $REF_FASTA $QRY_FASTA\n",
    "\n",
    "\t\t$MUMMER_BIN/dnadiff -p alignments/$REF_NAME/$QRY_NAME/nucmer -d alignments/$REF_NAME/$QRY_NAME/nucmer.delta\n",
    "\n",
    "\t\t$MUMMER_BIN/mummerplot -p alignments/$REF_NAME/$QRY_NAME/nucmer -s large --SNP --postscript --filter alignments/$REF_NAME/$QRY_NAME/nucmer.delta\n",
    "\t\t$MUMMER_BIN/mummerplot -p alignments/$REF_NAME/$QRY_NAME/nucmer -s large --SNP --png --filter alignments/$REF_NAME/$QRY_NAME/nucmer.delta\n",
    "\n",
    "\t\tmkdir alignments/$REF_NAME/$QRY_NAME/individual_alignments\n",
    "\n",
    "\t\tgrep \">\" alignments/$REF_NAME/$QRY_NAME/nucmer.delta | cut -d ' ' -f1,2 | sed 's/>//g' | while read list\n",
    "\t\tdo\n",
    "\t\t\techo $list\n",
    "\t\t\toutname=`echo $list | sed 's/ /_/g'`\n",
    "\t\t\t$MUMMER_BIN/show-aligns alignments/$REF_NAME/$QRY_NAME/nucmer.delta $list > alignments/$REF_NAME/$QRY_NAME/individual_alignments/$outname.aln\n",
    "\t\tdone\n",
    "\n",
    "\t\t# Extract unaligned scaffolds to the reference\n",
    "\t\tcat alignments/$REF_NAME/$QRY_NAME/nucmer.unqry | cut -f1 > alignments/$REF_NAME/$QRY_NAME/nucmer.unqry.names\n",
    "\t\tseqtk subseq $QRY_FASTA alignments/$REF_NAME/$QRY_NAME/nucmer.unqry.names > alignments/$REF_NAME/$QRY_NAME/nucmer.unqry.fasta\n",
    "\t\trm alignments/$REF_NAME/$QRY_NAME/nucmer.unqry.names\n",
    "\tdone\n",
    "done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Variant annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Creating custom annotation database for SnpEff\n",
    "\n",
    "\n",
    "* SnpEff is another way of how to annotate a VCF file\n",
    "   * It is an equivalent of Ensembl VEP, ANNOVAR and others\n",
    "   * If you are confused about the outputs (like variant effect) you can check the The Sequence Ontology where they try to collect naming of the most common annotation tools for each annotation “feature” (for example a missense variant) \n",
    "* In case your organisms is not included in the SnpEff annotation database or you need a different genome version (check your genome with java -jar SnpEff.jar databases | grep -i killifish) you might create your own from gtf, gff or GenBank file\n",
    "   * More information about databases in SnpEff can be found here\n",
    "* You can see an example of building bacterial genome annotation in snpeff_custom.sh and the addition to the main config file in snpeff_custom.config\n",
    "   * Note: the config file contains change of a standard codon table which might not be necessary for your organism! See the manual bellow \n",
    "\n",
    "\n",
    "1. Modify/create a config file with new genome specifications \n",
    "   * Go to SnpEff directory and edit the config file\n",
    "vi SnpEffect.config\n",
    "   * Add lines corresponding to the newly added genome, for example\n",
    "# Mouse genome, version mm37.61\n",
    "mm37.61.genome : Mouse\n",
    "   * If you need, you can also modify the codon table for individual chromosomes of the genome\n",
    "   * This is useful when you want to add bacterial genomes or genomes which have non-standard  (=vertebrae) codon table\n",
    "   * You can modify each chromosome separately\n",
    "1. Create a directory with genome annotation \n",
    "   * Go to the SnpEff directory and then to data\n",
    "cd /path/to/snpEff/data\n",
    "   * Create a directory with a same name as you used in the config file before “.genome”\n",
    "mkdir mm37.61\n",
    "1. Download the annotation and genome\n",
    "   * Go to the newly created folder and download both gene annotation (gtf, gff, UCSC/RefSeq or GenBank) and genome\n",
    "   * Remember to use the highlighted files names! In case of the annotation the file should be called genes.gtf.gz and should be placed in the created directory\n",
    "   * In case of a genome the file should be placed in /path/to/snpEff/data/genomes directory and should have the same name as the genome name in the config/annotation folder\n",
    "### Annotation\n",
    "$ cd mm37.61\n",
    "$ wget ftp://ftp.ensembl.org/pub/current/gtf/mus_musculus/Mus_musculus.NCBIM37.61.gtf.gz\n",
    "$ mv Mus_musculus.NCBIM37.61.gtf.gz genes.gtf.gz\n",
    "$ gunzip genes.gtf.gz\n",
    "### Genome\n",
    "$ cd /path/to/snpEff/data/genomes\n",
    "$ wget ftp://ftp.ensembl.org/pub/current/fasta/mus_musculus/dna/Mus_musculus.NCBIM37.61.dna.toplevel.fa.gz\n",
    "$ mv Mus_musculus.NCBIM37.61.dna.toplevel.fa.gz mm37.61.fa.gz\n",
    "$ gunzip mm37.61.fa.gz\n",
    "   * The genome could be also placed in the newly created folder together with the annotation (/path/to/snpEff/data/mm37.61) but then it has to be named sequences.fa\n",
    "   * In case you have a GenBank file (please check the manual how exactly to download it) you don’t have to download the genome, the genome download applies only to gtf or gff annotations\n",
    "1. Build the database\n",
    "   * Now you can build the database\n",
    "   * The build give you quite a lot of verbal outputs so you can check whether everything is OK\n",
    "   * For gtf annotation you can use following (again remember to use the same genome name as in the config file, annotation folder and/or genome fasta name\n",
    "$ cd /path/to/snpEff\n",
    "$ java -jar snpEff.jar build -gtf22 -v mm37.61\n",
    "1. Check the database is build OK and it contains everything you need\n",
    "   * To check a database/genome build you can use the dump command and see everything associated with the database\n",
    "$ java -jar snpEff.jar dump mm37.61 | less\n",
    "1. Run snpEff annotation\n",
    "   * Now you can happily annotate you VCF!\n",
    "$ java -jar snpEff.jar mm37.61 in.vcf > out.vcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#\n",
    "# SnpEff annotation with a custom database\n",
    "#\n",
    "# Designed for Treponema pallidum ssp. pallidum strain SS14 (CP004011) and Treponema pallidum ssp. pallidum strain Nichols (CP004010)\n",
    "# These genome assemblies might be different from those available for SnpEff annotation so we build the database from scratch\n",
    "# Treponema should have: Translation table 11: The bacterial, archaeal and plant plastid code ( https://en.wikipedia.org/wiki/Bacterial,_archaeal_and_plant_plastid_code)\n",
    "#\n",
    "################################################################################\n",
    "### Variables and binaries\n",
    "# Variables\n",
    "#REF_DIR=/home/jan/Data/projects/linda/2018_mar/results/annotation/data/SS14_CP004011.1 # Where you have the refences\n",
    "REF_DIR=/home/jan/Data/projects/linda/2018_mar/results/annotation/data/Nichols_CP004010.2 \n",
    "#REF_SEQ=sequence.fasta # Reference genome\n",
    "REF_ANNOT=sequence.gb # Reference gene annotation in GenBank (full) format with \"Show sequence\" on (don't forget to \"Update View\"!!!); with GenBank (full) you don't need to donwload the genome fasta file because it's already included\n",
    "SNPEFF_CONFIG=${REF_DIR}/snpEffect.config # Addition to the main snpEff.config file - be careful with this\n",
    "\n",
    "# Binaries\n",
    "SNPEFF_DIR=/home/jan/Tools/snpEff-4.2 \n",
    "OUTPUT_DIR=$SNPEFF_DIR/data/$(basename $REF_DIR) \n",
    "SNPEFF_RUN=\"java -Xmx2g -jar snpEff.jar\"\n",
    "\n",
    "################################################################################\n",
    "### Make the annotation dabase - only once per genome!\n",
    "if [ ! -d \"$OUTPUT_DIR\" ]; then  # Check if the genome folder already exists\n",
    "\n",
    "\tmkdir $OUTPUT_DIR\n",
    "\tcd $OUTPUT_DIR/\n",
    "\n",
    "\tcat $REF_DIR/$REF_ANNOT > $OUTPUT_DIR/genes.gbk\n",
    "\tcat $SNPEFF_CONFIG >> $SNPEFF_DIR/snpEff.config\n",
    "\n",
    "\tcd $SNPEFF_DIR/\n",
    "\t$SNPEFF_RUN build -genbank -v $(basename $REF_DIR)\n",
    "\n",
    "fi\n",
    "\n",
    "# Check if everything is OK \n",
    "$SNPEFF_RUN dump $(basename $REF_DIR) | less\n",
    "\n",
    "################################################################################\n",
    "### Running snpEff to evaluate the impact of SNPs\n",
    "INPUT_DIR=/home/jan/Data/projects/linda/2018_mar/results/annotation/vcf\n",
    "#INPUT_VCF=/home/jan/Data/projects/linda/2018_mar/results/annotation/vcf/test.vcf # Set the input VCF file\n",
    "cd $SNPEFF_DIR/\n",
    "\n",
    "for INPUT_VCF in $INPUT_DIR/*.vcf\n",
    "do\n",
    "\techo $INPUT_VCF\n",
    "\t$SNPEFF_RUN $(basename $REF_DIR) $INPUT_VCF > ${INPUT_VCF%.vcf}.$(basename $REF_DIR).annot.vcf # Run SnpEff\n",
    "done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
